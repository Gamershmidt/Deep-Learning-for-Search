{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa51ae32-6670-43d0-ae2e-ecb5da5d226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/sofia/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sofia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c87a586e-1d0b-4c6a-b67d-50c3dfcbc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets = [(tweet, 'positive') for tweet in positive_tweets]\n",
    "negative_tweets = [(tweet, 'negative') for tweet in negative_tweets]\n",
    "\n",
    "tweets = positive_tweets + negative_tweets\n",
    "random.shuffle(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bca272b-0daf-4004-8e36-79ddc0cda59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "all_words = nltk.FreqDist(word for tweet in tweets for word in preprocess_tweet(tweet[0]))\n",
    "most_common_words = list(all_words)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "312304c3-2bb5-4d3a-a6ba-51d7a9e299d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_features(tweet):\n",
    "    words = set(tokenize_tweet(tweet))\n",
    "    features = {}\n",
    "    for word in most_common_words:\n",
    "        features[f'contains({word})'] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7078b491-b479-41de-aab2-c7050d05e35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_sets \u001b[38;5;241m=\u001b[39m [(tweet_features(tweet), label) \u001b[38;5;28;01mfor\u001b[39;00m (tweet, label) \u001b[38;5;129;01min\u001b[39;00m tweets]\n\u001b[1;32m      3\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m feature_sets[:\u001b[38;5;241m7000\u001b[39m], feature_sets[\u001b[38;5;241m7000\u001b[39m:]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "feature_sets = [(tweet_features(tweet), label) for (tweet, label) in tweets]\n",
    "\n",
    "train_set, test_set = feature_sets[:7000], feature_sets[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15f0a40f-dcbc-4fa6-a57e-22802f4833c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ec216d6-e919-4012-b8d1-c6b75ca4bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9923333333333333\n",
      "Most Informative Features\n",
      "             contains()) = True           positi : negati =     79.6 : 1.0\n",
      "             contains(() = True           negati : positi =     73.6 : 1.0\n",
      "           contains(sad) = True           negati : positi =     50.2 : 1.0\n",
      "          contains(miss) = True           negati : positi =     34.5 : 1.0\n",
      "       contains(arrived) = True           positi : negati =     16.6 : 1.0\n",
      "       contains(welcome) = True           positi : negati =     13.8 : 1.0\n",
      "        contains(thanks) = True           positi : negati =     12.6 : 1.0\n",
      "         contains(didnt) = True           negati : positi =     12.3 : 1.0\n",
      "          contains(lost) = True           negati : positi =     11.6 : 1.0\n",
      "    contains(bestfriend) = True           positi : negati =     10.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy(classifier, test_set)}')\n",
    "\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2f4600d3-0b1d-4732-bba7-4d3727555679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweet(tweet):\n",
    "    features = tweet_features(tweet)\n",
    "    return classifier.classify(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60f74f65-5e4f-441a-8de3-a98e035d2a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: \"I love this movie! It's amazing.\" -> Sentiment: positive\n",
      "Tweet: \"I hate this product. It doesn't work at all.\" -> Sentiment: positive\n",
      "Tweet: \"What a fantastic day!\" -> Sentiment: positive\n",
      "Tweet: \"I'm so sad and depressed right now.\" -> Sentiment: positive\n",
      "Tweet: \"This is the worst experience I've ever had.\" -> Sentiment: positive\n",
      "Tweet: \"I'm feeling great today!\" -> Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "example_tweets = [\n",
    "    \"I love this movie! It's amazing.\",\n",
    "    \"I hate this product. It doesn't work at all.\",\n",
    "    \"What a fantastic day!\",\n",
    "    \"I'm so sad and depressed right now.\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"I'm feeling great today!\"\n",
    "]\n",
    "\n",
    "for tweet in example_tweets:\n",
    "    print(f'Tweet: \"{tweet}\" -> Sentiment: {classify_tweet(tweet)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c0241-31d2-491a-928c-edc7f6698e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
