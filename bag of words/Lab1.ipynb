{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986478b3-dd2c-460a-8a75-0f3d7fec28d6",
   "metadata": {},
   "source": [
    "Read the .pdf files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376e2a63-4eb5-4d17-a457-faca6ec1d03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b947f-ae08-497f-8cd3-d7e1437a4e52",
   "metadata": {},
   "source": [
    "### Step 1:  Implement word-based or subwordy tokenization\n",
    "\n",
    "The following code was taken from https://pymupdf.readthedocs.io/en/latest/the-basics.html.\n",
    "\n",
    "I have chosen library pymupdf since it works correctly with ligatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0104377d-b60a-4bb8-997f-f3a40c52afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "doc = pymupdf.open(\"paper1.pdf\") # open a document\n",
    "out = open(\"output.txt\", \"wb\") # create a text output\n",
    "for page in doc: # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "    out.write(text) # write text of page\n",
    "    out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b0804f-aa5f-49b5-88a8-523fc792f801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:1310.4546v1  [cs.CL]  16 Oct 2013\\nDistributed Representations of Words and Phrases\\nand their Compositionality\\nTomas Mikolov\\nGoogle Inc.\\nMountain View\\nmikolov@google.com\\nIlya Sutskever\\nGoogle Inc.\\nMountain View\\nilyasu@google.com\\nKai Chen\\nGoogle Inc.\\nMountain View\\nkai@google.com\\nGreg Corrado\\nGoogle Inc.\\nMountain View\\ngcorrado@google.com\\nJeffrey Dean\\nGoogle Inc.\\nMountain View\\njeff@google.com\\nAbstract\\nThe recently introduced continuous Skip-gram model is an efﬁcient method for\\nlearning high-quality distributed vector representations that capture a large num-\\nber of precise syntactic and semantic word relationships. In this paper we present\\nseveral extensions that improve both the quality of the vectors and the training\\nspeed. By subsampling of the frequent words we obtain signiﬁcant speedup and\\nalso learn more regular word representations. We also describe a simple alterna-\\ntive to the hierarchical softmax called negative sampling.\\nAn inherent limitation of word representations is their indifference to word order\\nand their inability to represent idiomatic phrases. For example, the meanings of\\n“Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated\\nby this example, we present a simple method for ﬁnding phrases in text, and show\\nthat learning good vector representations for millions of phrases is possible.\\n1\\nIntroduction\\nDistributed representations of words in a vector space help learning algorithms to achieve better\\nperformance in natural language processing tasks by grouping similar words. One of the earliest use\\nof word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea\\nhas since been applied to statistical language modeling with considerable success [1]. The follow\\nup work includes applications to automatic speech recognition and machine translation [14, 7], and\\na wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].\\nRecently, Mikolov et al. [8] introduced the Skip-gram model, an efﬁcient method for learning high-\\nquality vector representations of words from large amounts of unstructured text data. Unlike most\\nof the previously used neural network architectures for learning word vectors, training of the Skip-\\ngram model (see Figure 1) does not involve dense matrix multiplications. This makes the training\\nextremely efﬁcient: an optimized single-machine implementation can train on more than 100 billion\\nwords in one day.\\nThe word representations computed using neural networks are very interesting because the learned\\nvectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of\\nthese patterns can be represented as linear translations. For example, the result of a vector calcula-\\ntion vec(“Madrid”) - vec(“Spain”) + vec(“France”) is closer to vec(“Paris”) than to any other word\\nvector [9, 8].\\n1\\n\\x0c\\x01\\x02\\x03\\x04\\n\\x05\\x06\\x07\\x08\\x03\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\x07\\n\\x0b\\x0c\\r\\x0e\\x03\\x0f\\x0b\\x06\\t\\t\\t\\t\\t\\t\\x0b\\x08\\x03\\x07\\x08\\x03\\n\\x01\\x02\\x03\\x10\\x11\\x04\\n\\x01\\x02\\x03\\x10\\x12\\x04\\n\\x01\\x02\\x03\\x13\\x12\\x04\\n\\x01\\x02\\x03\\x13\\x11\\x04\\nFigure 1: The Skip-gram model architecture. The training objective is to learn word vector representations\\nthat are good at predicting the nearby words.\\nIn this paper we present several extensions of the original Skip-gram model. We show that sub-\\nsampling of frequent words during training results in a signiﬁcant speedup (around 2x - 10x), and\\nimproves accuracy of the representations of less frequent words. In addition, we present a simpli-\\nﬁed variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results\\nin faster training and better vector representations for frequent words, compared to more complex\\nhierarchical softmax that was used in the prior work [8].\\nWord representations are limited by their inability to represent idiomatic phrases that are not com-\\npositions of the individual words. For example, “Boston Globe” is a newspaper, and so it is not a\\nnatural combination of the meanings of “Boston” and “Globe”. Therefore, using vectors to repre-\\nsent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques\\nthat aim to represent meaning of sentences by composing the word vectors, such as the recursive\\nautoencoders [15], would also beneﬁt from using phrase vectors instead of the word vectors.\\nThe extension from word based to phrase based models is relatively simple. First we identify a large\\nnumber of phrases using a data-driven approach, and then we treat the phrases as individual tokens\\nduring the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi-\\ncal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is\\n“Montreal”:“Montreal Canadiens”::“Toronto”:“Toronto Maple Leafs”. It is considered to have been\\nanswered correctly if the nearest representation to vec(“Montreal Canadiens”) - vec(“Montreal”) +\\nvec(“Toronto”) is vec(“Toronto Maple Leafs”).\\nFinally, we describe another interesting property of the Skip-gram model. We found that simple\\nvector addition can often produce meaningful results. For example, vec(“Russia”) + vec(“river”) is\\nclose to vec(“Volga River”), and vec(“Germany”) + vec(“capital”) is close to vec(“Berlin”). This\\ncompositionality suggests that a non-obvious degree of language understanding can be obtained by\\nusing basic mathematical operations on the word vector representations.\\n2\\nThe Skip-gram Model\\nThe training objective of the Skip-gram model is to ﬁnd word representations that are useful for\\npredicting the surrounding words in a sentence or a document. More formally, given a sequence of\\ntraining words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average\\nlog probability\\n1\\nT\\nT\\nX\\nt=1\\nX\\n−c≤j≤c,j̸=0\\nlog p(wt+j|wt)\\n(1)\\nwhere c is the size of the training context (which can be a function of the center word wt). Larger\\nc results in more training examples and thus can lead to a higher accuracy, at the expense of the\\n2\\n\\x0ctraining time. The basic Skip-gram formulation deﬁnes p(wt+j|wt) using the softmax function:\\np(wO|wI) =\\nexp\\n\\x10\\nv′\\nwO\\n⊤vwI\\n\\x11\\nPW\\nw=1 exp\\n\\x10\\nv′\\nw\\n⊤vwI\\n\\x11\\n(2)\\nwhere vw and v′\\nw are the “input” and “output” vector representations of w, and W is the num-\\nber of words in the vocabulary. This formulation is impractical because the cost of computing\\n∇log p(wO|wI) is proportional to W, which is often large (105–107 terms).\\n2.1\\nHierarchical Softmax\\nA computationally efﬁcient approximation of the full softmax is the hierarchical softmax. In the\\ncontext of neural network language models, it was ﬁrst introduced by Morin and Bengio [12]. The\\nmain advantage is that instead of evaluating W output nodes in the neural network to obtain the\\nprobability distribution, it is needed to evaluate only about log2(W) nodes.\\nThe hierarchical softmax uses a binary tree representation of the output layer with the W words as\\nits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These\\ndeﬁne a random walk that assigns probabilities to words.\\nMore precisely, each word w can be reached by an appropriate path from the root of the tree. Let\\nn(w, j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so\\nn(w, 1) = root and n(w, L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary\\nﬁxed child of n and let [\\n[x]\\n] be 1 if x is true and -1 otherwise. Then the hierarchical softmax deﬁnes\\np(wO|wI) as follows:\\np(w|wI) =\\nL(w)−1\\nY\\nj=1\\nσ\\n\\x10\\n[\\n[n(w, j + 1) = ch(n(w, j))]\\n] · v′\\nn(w,j)\\n⊤vwI\\n\\x11\\n(3)\\nwhere σ(x) = 1/(1 + exp(−x)). It can be veriﬁed that PW\\nw=1 p(w|wI) = 1. This implies that the\\ncost of computing log p(wO|wI) and ∇log p(wO|wI) is proportional to L(wO), which on average\\nis no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which\\nassigns two representations vw and v′\\nw to each word w, the hierarchical softmax formulation has\\none representation vw for each word w and one representation v′\\nn for every inner node n of the\\nbinary tree.\\nThe structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-\\nmance. Mnih and Hinton explored a number of methods for constructing the tree structure and the\\neffect on both the training time and the resulting model accuracy [10]. In our work we use a binary\\nHuffman tree, as it assigns short codes to the frequent words which results in fast training. It has\\nbeen observed before that grouping words together by their frequency works well as a very simple\\nspeedup technique for the neural network based language models [5, 8].\\n2.2\\nNegative Sampling\\nAn alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-\\ntroduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].\\nNCE posits that a good model should be able to differentiate data from noise by means of logistic\\nregression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models\\nby ranking the data above noise.\\nWhile NCE can be shown to approximately maximize the log probability of the softmax, the Skip-\\ngram model is only concerned with learning high-quality vector representations, so we are free to\\nsimplify NCE as long as the vector representations retain their quality. We deﬁne Negative sampling\\n(NEG) by the objective\\nlog σ(v′\\nwO\\n⊤vwI) +\\nk\\nX\\ni=1\\nEwi∼Pn(w)\\nh\\nlog σ(−v′\\nwi\\n⊤vwI)\\ni\\n(4)\\n3\\n\\x0c-2\\n-1.5\\n-1\\n-0.5\\n 0\\n 0.5\\n 1\\n 1.5\\n 2\\n-2\\n-1.5\\n-1\\n-0.5\\n 0\\n 0.5\\n 1\\n 1.5\\n 2\\nCountry and Capital Vectors Projected by PCA\\nChina\\nJapan\\nFrance\\nRussia\\nGermany\\nItaly\\nSpain\\nGreece\\nTurkey\\nBeijing\\nParis\\nTokyo\\nPoland\\nMoscow\\nPortugal\\nBerlin\\nRome\\nAthens\\nMadrid\\nAnkara\\nWarsaw\\nLisbon\\nFigure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their\\ncapital cities. The ﬁgure illustrates ability of the model to automatically organize concepts and learn implicitly\\nthe relationships between them, as during the training we did not provide any supervised information about\\nwhat a capital city means.\\nwhich is used to replace every log P(wO|wI) term in the Skip-gram objective. Thus the task is to\\ndistinguish the target word wO from draws from the noise distribution Pn(w) using logistic regres-\\nsion, where there are k negative samples for each data sample. Our experiments indicate that values\\nof k in the range 5–20 are useful for small training datasets, while for large datasets the k can be as\\nsmall as 2–5. The main difference between the Negative sampling and NCE is that NCE needs both\\nsamples and the numerical probabilities of the noise distribution, while Negative sampling uses only\\nsamples. And while NCE approximately maximizes the log probability of the softmax, this property\\nis not important for our application.\\nBoth NCE and NEG have the noise distribution Pn(w) as a free parameter. We investigated a number\\nof choices for Pn(w) and found that the unigram distribution U(w) raised to the 3/4rd power (i.e.,\\nU(w)3/4/Z) outperformed signiﬁcantly the unigram and the uniform distributions, for both NCE\\nand NEG on every task we tried including language modeling (not reported here).\\n2.3\\nSubsampling of Frequent Words\\nIn very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,\\n“in”, “the”, and “a”). Such words usually provide less information value than the rare words. For\\nexample, while the Skip-gram model beneﬁts from observing the co-occurrences of “France” and\\n“Paris”, it beneﬁts much less from observing the frequent co-occurrences of “France” and “the”, as\\nnearly every word co-occurs frequently within a sentence with “the”. This idea can also be applied\\nin the opposite direction; the vector representations of frequent words do not change signiﬁcantly\\nafter training on several million examples.\\nTo counter the imbalance between the rare and frequent words, we used a simple subsampling ap-\\nproach: each word wi in the training set is discarded with probability computed by the formula\\nP(wi) = 1 −\\ns\\nt\\nf(wi)\\n(5)\\n4\\n\\x0cMethod\\nTime [min]\\nSyntactic [%]\\nSemantic [%]\\nTotal accuracy [%]\\nNEG-5\\n38\\n63\\n54\\n59\\nNEG-15\\n97\\n63\\n58\\n61\\nHS-Huffman\\n41\\n53\\n40\\n47\\nNCE-5\\n38\\n60\\n45\\n53\\nThe following results use 10−5 subsampling\\nNEG-5\\n14\\n61\\n58\\n60\\nNEG-15\\n36\\n61\\n61\\n61\\nHS-Huffman\\n21\\n52\\n59\\n55\\nTable 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task\\nas deﬁned in [8]. NEG-k stands for Negative Sampling with k negative samples for each positive\\nsample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the Hierarchical\\nSoftmax with the frequency-based Huffman codes.\\nwhere f(wi) is the frequency of word wi and t is a chosen threshold, typically around 10−5.\\nWe chose this subsampling formula because it aggressively subsamples words whose frequency\\nis greater than t while preserving the ranking of the frequencies. Although this subsampling for-\\nmula was chosen heuristically, we found it to work well in practice. It accelerates learning and even\\nsigniﬁcantly improves the accuracy of the learned vectors of the rare words, as will be shown in the\\nfollowing sections.\\n3\\nEmpirical Results\\nIn this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative\\nSampling, and subsampling of the training words. We used the analogical reasoning task1 introduced\\nby Mikolov et al. [8]. The task consists of analogies such as “Germany” : “Berlin” :: “France” : ?,\\nwhich are solved by ﬁnding a vector x such that vec(x) is closest to vec(“Berlin”) - vec(“Germany”)\\n+ vec(“France”) according to the cosine distance (we discard the input words from the search). This\\nspeciﬁc example is considered to have been answered correctly if x is “Paris”. The task has two\\nbroad categories: the syntactic analogies (such as “quick” : “quickly” :: “slow” : “slowly”) and the\\nsemantic analogies, such as the country to capital city relationship.\\nFor training the Skip-gram models, we have used a large dataset consisting of various news articles\\n(an internal Google dataset with one billion words). We discarded from the vocabulary all words\\nthat occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.\\nThe performance of various Skip-gram models on the word analogy test set is reported in Table 1.\\nThe table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical\\nreasoning task, and has even slightly better performance than the Noise Contrastive Estimation. The\\nsubsampling of the frequent words improves the training speed several times and makes the word\\nrepresentations signiﬁcantly more accurate.\\nIt can be argued that the linearity of the skip-gram model makes its vectors more suitable for such\\nlinear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned\\nby the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this\\ntask signiﬁcantly as the amount of the training data increases, suggesting that non-linear models also\\nhave a preference for a linear structure of the word representations.\\n4\\nLearning Phrases\\nAs discussed earlier, many phrases have a meaning that is not a simple composition of the mean-\\nings of its individual words. To learn vector representation for phrases, we ﬁrst ﬁnd words that\\nappear frequently together, and infrequently in other contexts. For example, “New York Times” and\\n“Toronto Maple Leafs” are replaced by unique tokens in the training data, while a bigram “this is”\\nwill remain unchanged.\\n1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\\n5\\n\\x0cNewspapers\\nNew York\\nNew York Times\\nBaltimore\\nBaltimore Sun\\nSan Jose\\nSan Jose Mercury News\\nCincinnati\\nCincinnati Enquirer\\nNHL Teams\\nBoston\\nBoston Bruins\\nMontreal\\nMontreal Canadiens\\nPhoenix\\nPhoenix Coyotes\\nNashville\\nNashville Predators\\nNBA Teams\\nDetroit\\nDetroit Pistons\\nToronto\\nToronto Raptors\\nOakland\\nGolden State Warriors\\nMemphis\\nMemphis Grizzlies\\nAirlines\\nAustria\\nAustrian Airlines\\nSpain\\nSpainair\\nBelgium\\nBrussels Airlines\\nGreece\\nAegean Airlines\\nCompany executives\\nSteve Ballmer\\nMicrosoft\\nLarry Page\\nGoogle\\nSamuel J. Palmisano\\nIBM\\nWerner Vogels\\nAmazon\\nTable 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples).\\nThe goal is to compute the fourth phrase using the ﬁrst three. Our best model achieved an accuracy\\nof 72% on this dataset.\\nThis way, we can form many reasonable phrases without greatly increasing the size of the vocabu-\\nlary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory\\nintensive. Many techniques have been previously developed to identify phrases in the text; however,\\nit is out of scope of our work to compare them. We decided to use a simple data-driven approach,\\nwhere phrases are formed based on the unigram and bigram counts, using\\nscore(wi, wj) =\\ncount(wiwj) −δ\\ncount(wi) × count(wj).\\n(6)\\nThe δ is used as a discounting coefﬁcient and prevents too many phrases consisting of very infre-\\nquent words to be formed. The bigrams with score above the chosen threshold are then used as\\nphrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow-\\ning longer phrases that consists of several words to be formed. We evaluate the quality of the phrase\\nrepresentations using a new analogical reasoning task that involves phrases. Table 2 shows examples\\nof the ﬁve categories of analogies used in this task. This dataset is publicly available on the web2.\\n4.1\\nPhrase Skip-Gram Results\\nStarting with the same news data as in the previous experiments, we ﬁrst constructed the phrase\\nbased training corpus and then we trained several Skip-gram models using different hyper-\\nparameters. As before, we used vector dimensionality 300 and context size 5. This setting already\\nachieves good performance on the phrase dataset, and allowed us to quickly compare the Negative\\nSampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.\\nThe results are summarized in Table 3.\\nThe results show that while Negative Sampling achieves a respectable accuracy even with k = 5,\\nusing k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-\\nchical Softmax to achieve lower performance when trained without subsampling, it became the best\\nperforming method when we downsampled the frequent words. This shows that the subsampling\\ncan result in faster training and can also improve accuracy, at least in some cases.\\n2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt\\nMethod\\nDimensionality\\nNo subsampling [%]\\n10−5 subsampling [%]\\nNEG-5\\n300\\n24\\n27\\nNEG-15\\n300\\n27\\n42\\nHS-Huffman\\n300\\n19\\n47\\nTable 3:\\nAccuracies of the Skip-gram models on the phrase analogy dataset. The models were\\ntrained on approximately one billion words from the news dataset.\\n6\\n\\x0cNEG-15 with 10−5 subsampling\\nHS with 10−5 subsampling\\nVasco de Gama\\nLingsugur\\nItalian explorer\\nLake Baikal\\nGreat Rift Valley\\nAral Sea\\nAlan Bean\\nRebbeca Naomi\\nmoonwalker\\nIonian Sea\\nRuegen\\nIonian Islands\\nchess master\\nchess grandmaster\\nGarry Kasparov\\nTable 4: Examples of the closest entities to the given short phrases, using two different models.\\nCzech + currency\\nVietnam + capital\\nGerman + airlines\\nRussian + river\\nFrench + actress\\nkoruna\\nHanoi\\nairline Lufthansa\\nMoscow\\nJuliette Binoche\\nCheck crown\\nHo Chi Minh City\\ncarrier Lufthansa\\nVolga River\\nVanessa Paradis\\nPolish zolty\\nViet Nam\\nﬂag carrier Lufthansa\\nupriver\\nCharlotte Gainsbourg\\nCTK\\nVietnamese\\nLufthansa\\nRussia\\nCecile De\\nTable 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two\\nvectors are shown, using the best Skip-gram model.\\nTo maximize the accuracy on the phrase analogy task, we increased the amount of the training data\\nby using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality\\nof 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy\\nof 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B\\nwords, which suggests that the large amount of the training data is crucial.\\nTo gain further insight into how different the representations learned by different models are, we did\\ninspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we\\nshow a sample of such comparison. Consistently with the previous results, it seems that the best\\nrepresentations of phrases are learned by a model with the hierarchical softmax and subsampling.\\n5\\nAdditive Compositionality\\nWe demonstrated that the word and phrase representations learned by the Skip-gram model exhibit\\na linear structure that makes it possible to perform precise analogical reasoning using simple vector\\narithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear\\nstructure that makes it possible to meaningfully combine words by an element-wise addition of their\\nvector representations. This phenomenon is illustrated in Table 5.\\nThe additive property of the vectors can be explained by inspecting the training objective. The word\\nvectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors\\nare trained to predict the surrounding words in the sentence, the vectors can be seen as representing\\nthe distribution of the context in which a word appears. These values are related logarithmically\\nto the probabilities computed by the output layer, so the sum of two word vectors is related to the\\nproduct of the two context distributions. The product works here as the AND function: words that\\nare assigned high probabilities by both word vectors will have high probability, and the other words\\nwill have low probability. Thus, if “Volga River” appears frequently in the same sentence together\\nwith the words “Russian” and “river”, the sum of these two word vectors will result in such a feature\\nvector that is close to the vector of “Volga River”.\\n6\\nComparison to Published Word Representations\\nMany authors who previously worked on the neural network based representations of words have\\npublished their resulting models for further use and comparison: amongst the most well known au-\\nthors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded\\ntheir word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa-\\ntions on the word analogy task, where the Skip-gram models achieved the best performance with a\\nhuge margin.\\n3http://metaoptimize.com/projects/wordreprs/\\n7\\n\\x0cModel\\nRedmond\\nHavel\\nninjutsu\\ngrafﬁti\\ncapitulate\\n(training time)\\nCollobert (50d)\\nconyers\\nplauen\\nreiki\\ncheesecake\\nabdicate\\n(2 months)\\nlubbock\\ndzerzhinsky\\nkohona\\ngossip\\naccede\\nkeene\\nosterreich\\nkarate\\ndioramas\\nrearm\\nTurian (200d)\\nMcCarthy\\nJewell\\n-\\ngunﬁre\\n-\\n(few weeks)\\nAlston\\nArzu\\n-\\nemotion\\n-\\nCousins\\nOvitz\\n-\\nimpunity\\n-\\nMnih (100d)\\nPodhurst\\nPontiff\\n-\\nanaesthetics\\nMavericks\\n(7 days)\\nHarlang\\nPinochet\\n-\\nmonkeys\\nplanning\\nAgarwal\\nRodionov\\n-\\nJews\\nhesitated\\nSkip-Phrase\\nRedmond Wash.\\nVaclav Havel\\nninja\\nspray paint\\ncapitulation\\n(1000d, 1 day)\\nRedmond Washington\\npresident Vaclav Havel\\nmartial arts\\ngraﬁtti\\ncapitulated\\nMicrosoft\\nVelvet Revolution\\nswordsmanship\\ntaggers\\ncapitulating\\nTable 6: Examples of the closest tokens given various well known models and the Skip-gram model\\ntrained on phrases using over 30 billion training words. An empty cell means that the word was not\\nin the vocabulary.\\nTo give more insight into the difference of the quality of the learned vectors, we provide empirical\\ncomparison by showing the nearest neighbours of infrequent words in Table 6. These examples show\\nthat the big Skip-gram model trained on a large corpus visibly outperforms all the other models in\\nthe quality of the learned representations. This can be attributed in part to the fact that this model\\nhas been trained on about 30 billion words, which is about two to three orders of magnitude more\\ndata than the typical size used in the prior work. Interestingly, although the training set is much\\nlarger, the training time of the Skip-gram model is just a fraction of the time complexity required by\\nthe previous model architectures.\\n7\\nConclusion\\nThis work has several key contributions. We show how to train distributed representations of words\\nand phrases with the Skip-gram model and demonstrate that these representations exhibit linear\\nstructure that makes precise analogical reasoning possible. The techniques introduced in this paper\\ncan be used also for training the continuous bag-of-words model introduced in [8].\\nWe successfully trained models on several orders of magnitude more data than the previously pub-\\nlished models, thanks to the computationally efﬁcient model architecture. This results in a great\\nimprovement in the quality of the learned word and phrase representations, especially for the rare\\nentities. We also found that the subsampling of the frequent words results in both faster training\\nand signiﬁcantly better representations of uncommon words. Another contribution of our paper is\\nthe Negative sampling algorithm, which is an extremely simple training method that learns accurate\\nrepresentations especially for frequent words.\\nThe choice of the training algorithm and the hyper-parameter selection is a task speciﬁc decision,\\nas we found that different problems have different optimal hyperparameter conﬁgurations. In our\\nexperiments, the most crucial decisions that affect the performance are the choice of the model\\narchitecture, the size of the vectors, the subsampling rate, and the size of the training window.\\nA very interesting result of this work is that the word vectors can be somewhat meaningfully com-\\nbined using just simple vector addition. Another approach for learning representations of phrases\\npresented in this paper is to simply represent the phrases with a single token. Combination of these\\ntwo approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-\\ning minimal computational complexity. Our work can thus be seen as complementary to the existing\\napproach that attempts to represent phrases using recursive matrix-vector operations [16].\\nWe made the code for training the word and phrase vectors based on the techniques described in this\\npaper available as an open-source project4.\\n4code.google.com/p/word2vec\\n8\\n\\x0cReferences\\n[1] Yoshua Bengio, R´\\nejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language\\nmodel. The Journal of Machine Learning Research, 3:1137–1155, 2003.\\n[2] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neu-\\nral networks with multitask learning. In Proceedings of the 25th international conference on Machine\\nlearning, pages 160–167. ACM, 2008.\\n[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-\\nﬁcation: A deep learning approach. In ICML, 513–520, 2011.\\n[4] Michael U Gutmann and Aapo Hyv¨\\narinen. Noise-contrastive estimation of unnormalized statistical mod-\\nels, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307–361,\\n2012.\\n[5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of\\nrecurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011\\nIEEE International Conference on, pages 5528–5531. IEEE, 2011.\\n[6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training\\nLarge Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand-\\ning, 2011.\\n[7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno\\nUniversity of Technology, 2012.\\n[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations\\nin vector space. ICLR Workshop, 2013.\\n[9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word\\nRepresentations. In Proceedings of NAACL HLT, 2013.\\n[10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in\\nneural information processing systems, 21:1081–1088, 2009.\\n[11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language\\nmodels. arXiv preprint arXiv:1206.6426, 2012.\\n[12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-\\nceedings of the international workshop on artiﬁcial intelligence and statistics, pages 246–252, 2005.\\n[13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-\\npropagating errors. Nature, 323(6088):533–536, 1986.\\n[14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.\\n[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and\\nnatural language with recursive neural networks. In Proceedings of the 26th International Conference on\\nMachine Learning (ICML), volume 2, 2011.\\n[16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality\\nThrough Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP), 2012.\\n[17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for\\nsemi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-\\ntional Linguistics, pages 384–394. Association for Computational Linguistics, 2010.\\n[18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In\\nJournal of Artiﬁcial Intelligence Research, 37:141-188, 2010.\\n[19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\\nIn Transactions of the Association for Computational Linguistics (TACL), 353–366, 2013.\\n[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-\\ntion. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial Intelligence-Volume\\nVolume Three, pages 2764–2770. AAAI Press, 2011.\\n9\\n\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "with open(\"output.txt\", \"rb\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "decoded_content = content.decode(\"utf8\")\n",
    "decoded_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a27a10-aa32-4eb7-bedc-69a703042867",
   "metadata": {},
   "source": [
    "The following regular expression accepts words from a text and excludes words that are purely numeric, no special symbols included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285cf216-abc2-4357-b34d-51e7cd34237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arXiv', '4546v1', 'cs', 'CL', 'Oct', 'Distributed', 'Representations', 'of', 'Words', 'and', 'Phrases', 'and', 'their', 'Compositionality', 'Tomas', 'Mikolov', 'Google', 'Inc', 'Mountain', 'View', 'mikolov', 'google', 'com', 'Ilya', 'Sutskever', 'Google', 'Inc', 'Mountain', 'View', 'ilyasu', 'google', 'com', 'Kai', 'Chen', 'Google', 'Inc', 'Mountain', 'View', 'kai', 'google', 'com', 'Greg', 'Corrado', 'Google', 'Inc', 'Mountain', 'View', 'gcorrado', 'google', 'com', 'Jeffrey', 'Dean', 'Google', 'Inc', 'Mountain', 'View', 'jeff', 'google', 'com', 'Abstract', 'The', 'recently', 'introduced', 'continuous', 'Skip', 'gram', 'model', 'is', 'an', 'efﬁcient', 'method', 'for', 'learning', 'high', 'quality', 'distributed', 'vector', 'representations', 'that', 'capture', 'a', 'large', 'num', 'ber', 'of', 'precise', 'syntactic', 'and', 'semantic', 'word', 'relationships', 'In', 'this', 'paper', 'we', 'present', 'several', 'extensions', 'that', 'improve', 'both', 'the', 'quality', 'of', 'the', 'vectors', 'and', 'the', 'training', 'speed', 'By', 'subsampling', 'of', 'the', 'frequent', 'words', 'we', 'obtain', 'signiﬁcant', 'speedup', 'and', 'also', 'learn', 'more', 'regular', 'word', 'representations', 'We', 'also', 'describe', 'a', 'simple', 'alterna', 'tive', 'to', 'the', 'hierarchical', 'softmax', 'called', 'negative', 'sampling', 'An', 'inherent', 'limitation', 'of', 'word', 'representations', 'is', 'their', 'indifference', 'to', 'word', 'order', 'and', 'their', 'inability', 'to', 'represent', 'idiomatic', 'phrases', 'For', 'example', 'the', 'meanings', 'of', 'Canada', 'and', 'Air', 'cannot', 'be', 'easily', 'combined', 'to', 'obtain', 'Air', 'Canada', 'Motivated', 'by', 'this', 'example', 'we', 'present', 'a', 'simple', 'method', 'for', 'ﬁnding', 'phrases', 'in', 'text', 'and', 'show', 'that', 'learning', 'good', 'vector', 'representations', 'for', 'millions', 'of', 'phrases', 'is', 'possible', 'Introduction', 'Distributed', 'representations', 'of', 'words', 'in', 'a', 'vector', 'space', 'help', 'learning', 'algorithms', 'to', 'achieve', 'better', 'performance', 'in', 'natural', 'language', 'processing', 'tasks', 'by', 'grouping', 'similar', 'words', 'One', 'of', 'the', 'earliest', 'use', 'of', 'word', 'representations', 'dates', 'back', 'to', 'due', 'to', 'Rumelhart', 'Hinton', 'and', 'Williams', 'This', 'idea', 'has', 'since', 'been', 'applied', 'to', 'statistical', 'language', 'modeling', 'with', 'considerable', 'success', 'The', 'follow', 'up', 'work', 'includes', 'applications', 'to', 'automatic', 'speech', 'recognition', 'and', 'machine', 'translation', 'and', 'a', 'wide', 'range', 'of', 'NLP', 'tasks', 'Recently', 'Mikolov', 'et', 'al', 'introduced', 'the', 'Skip', 'gram', 'model', 'an', 'efﬁcient', 'method', 'for', 'learning', 'high', 'quality', 'vector', 'representations', 'of', 'words', 'from', 'large', 'amounts', 'of', 'unstructured', 'text', 'data', 'Unlike', 'most', 'of', 'the', 'previously', 'used', 'neural', 'network', 'architectures', 'for', 'learning', 'word', 'vectors', 'training', 'of', 'the', 'Skip', 'gram', 'model', 'see', 'Figure', 'does', 'not', 'involve', 'dense', 'matrix', 'multiplications', 'This', 'makes', 'the', 'training', 'extremely', 'efﬁcient', 'an', 'optimized', 'single', 'machine', 'implementation', 'can', 'train', 'on', 'more', 'than', 'billion', 'words', 'in', 'one', 'day', 'The', 'word', 'representations', 'computed', 'using', 'neural', 'networks', 'are', 'very', 'interesting', 'because', 'the', 'learned', 'vectors', 'explicitly', 'encode', 'many', 'linguistic', 'regularities', 'and', 'patterns', 'Somewhat', 'surprisingly', 'many', 'of', 'these', 'patterns', 'can', 'be', 'represented', 'as', 'linear', 'translations', 'For', 'example', 'the', 'result', 'of', 'a', 'vector', 'calcula', 'tion', 'vec', 'Madrid', 'vec', 'Spain', 'vec', 'France', 'is', 'closer', 'to', 'vec', 'Paris', 'than', 'to', 'any', 'other', 'word', 'vector', 'Figure', 'The', 'Skip', 'gram', 'model', 'architecture', 'The', 'training', 'objective', 'is', 'to', 'learn', 'word', 'vector', 'representations', 'that', 'are', 'good', 'at', 'predicting', 'the', 'nearby', 'words', 'In', 'this', 'paper', 'we', 'present', 'several', 'extensions', 'of', 'the', 'original', 'Skip', 'gram', 'model', 'We', 'show', 'that', 'sub', 'sampling', 'of', 'frequent', 'words', 'during', 'training', 'results', 'in', 'a', 'signiﬁcant', 'speedup', 'around', '2x', '10x', 'and', 'improves', 'accuracy', 'of', 'the', 'representations', 'of', 'less', 'frequent', 'words', 'In', 'addition', 'we', 'present', 'a', 'simpli', 'ﬁed', 'variant', 'of', 'Noise', 'Contrastive', 'Estimation', 'NCE', 'for', 'training', 'the', 'Skip', 'gram', 'model', 'that', 'results', 'in', 'faster', 'training', 'and', 'better', 'vector', 'representations', 'for', 'frequent', 'words', 'compared', 'to', 'more', 'complex', 'hierarchical', 'softmax', 'that', 'was', 'used', 'in', 'the', 'prior', 'work', 'Word', 'representations', 'are', 'limited', 'by', 'their', 'inability', 'to', 'represent', 'idiomatic', 'phrases', 'that', 'are', 'not', 'com', 'positions', 'of', 'the', 'individual', 'words', 'For', 'example', 'Boston', 'Globe', 'is', 'a', 'newspaper', 'and', 'so', 'it', 'is', 'not', 'a', 'natural', 'combination', 'of', 'the', 'meanings', 'of', 'Boston', 'and', 'Globe', 'Therefore', 'using', 'vectors', 'to', 'repre', 'sent', 'the', 'whole', 'phrases', 'makes', 'the', 'Skip', 'gram', 'model', 'considerably', 'more', 'expressive', 'Other', 'techniques', 'that', 'aim', 'to', 'represent', 'meaning', 'of', 'sentences', 'by', 'composing', 'the', 'word', 'vectors', 'such', 'as', 'the', 'recursive', 'autoencoders', 'would', 'also', 'beneﬁt', 'from', 'using', 'phrase', 'vectors', 'instead', 'of', 'the', 'word', 'vectors', 'The', 'extension', 'from', 'word', 'based', 'to', 'phrase', 'based', 'models', 'is', 'relatively', 'simple', 'First', 'we', 'identify', 'a', 'large', 'number', 'of', 'phrases', 'using', 'a', 'data', 'driven', 'approach', 'and', 'then', 'we', 'treat', 'the', 'phrases', 'as', 'individual', 'tokens', 'during', 'the', 'training', 'To', 'evaluate', 'the', 'quality', 'of', 'the', 'phrase', 'vectors', 'we', 'developed', 'a', 'test', 'set', 'of', 'analogi', 'cal', 'reasoning', 'tasks', 'that', 'contains', 'both', 'words', 'and', 'phrases', 'A', 'typical', 'analogy', 'pair', 'from', 'our', 'test', 'set', 'is', 'Montreal', 'Montreal', 'Canadiens', 'Toronto', 'Toronto', 'Maple', 'Leafs', 'It', 'is', 'considered', 'to', 'have', 'been', 'answered', 'correctly', 'if', 'the', 'nearest', 'representation', 'to', 'vec', 'Montreal', 'Canadiens', 'vec', 'Montreal', 'vec', 'Toronto', 'is', 'vec', 'Toronto', 'Maple', 'Leafs', 'Finally', 'we', 'describe', 'another', 'interesting', 'property', 'of', 'the', 'Skip', 'gram', 'model', 'We', 'found', 'that', 'simple', 'vector', 'addition', 'can', 'often', 'produce', 'meaningful', 'results', 'For', 'example', 'vec', 'Russia', 'vec', 'river', 'is', 'close', 'to', 'vec', 'Volga', 'River', 'and', 'vec', 'Germany', 'vec', 'capital', 'is', 'close', 'to', 'vec', 'Berlin', 'This', 'compositionality', 'suggests', 'that', 'a', 'non', 'obvious', 'degree', 'of', 'language', 'understanding', 'can', 'be', 'obtained', 'by', 'using', 'basic', 'mathematical', 'operations', 'on', 'the', 'word', 'vector', 'representations', 'The', 'Skip', 'gram', 'Model', 'The', 'training', 'objective', 'of', 'the', 'Skip', 'gram', 'model', 'is', 'to', 'ﬁnd', 'word', 'representations', 'that', 'are', 'useful', 'for', 'predicting', 'the', 'surrounding', 'words', 'in', 'a', 'sentence', 'or', 'a', 'document', 'More', 'formally', 'given', 'a', 'sequence', 'of', 'training', 'words', 'w1', 'w2', 'w3', 'wT', 'the', 'objective', 'of', 'the', 'Skip', 'gram', 'model', 'is', 'to', 'maximize', 'the', 'average', 'log', 'probability', 'T', 'T', 'X', 't', 'X', 'c', 'j', 'c', 'j', 'log', 'p', 'wt', 'j', 'wt', 'where', 'c', 'is', 'the', 'size', 'of', 'the', 'training', 'context', 'which', 'can', 'be', 'a', 'function', 'of', 'the', 'center', 'word', 'wt', 'Larger', 'c', 'results', 'in', 'more', 'training', 'examples', 'and', 'thus', 'can', 'lead', 'to', 'a', 'higher', 'accuracy', 'at', 'the', 'expense', 'of', 'the', 'training', 'time', 'The', 'basic', 'Skip', 'gram', 'formulation', 'deﬁnes', 'p', 'wt', 'j', 'wt', 'using', 'the', 'softmax', 'function', 'p', 'wO', 'wI', 'exp', 'v', 'wO', 'vwI', 'PW', 'w', 'exp', 'v', 'w', 'vwI', 'where', 'vw', 'and', 'v', 'w', 'are', 'the', 'input', 'and', 'output', 'vector', 'representations', 'of', 'w', 'and', 'W', 'is', 'the', 'num', 'ber', 'of', 'words', 'in', 'the', 'vocabulary', 'This', 'formulation', 'is', 'impractical', 'because', 'the', 'cost', 'of', 'computing', 'log', 'p', 'wO', 'wI', 'is', 'proportional', 'to', 'W', 'which', 'is', 'often', 'large', 'terms', 'Hierarchical', 'Softmax', 'A', 'computationally', 'efﬁcient', 'approximation', 'of', 'the', 'full', 'softmax', 'is', 'the', 'hierarchical', 'softmax', 'In', 'the', 'context', 'of', 'neural', 'network', 'language', 'models', 'it', 'was', 'ﬁrst', 'introduced', 'by', 'Morin', 'and', 'Bengio', 'The', 'main', 'advantage', 'is', 'that', 'instead', 'of', 'evaluating', 'W', 'output', 'nodes', 'in', 'the', 'neural', 'network', 'to', 'obtain', 'the', 'probability', 'distribution', 'it', 'is', 'needed', 'to', 'evaluate', 'only', 'about', 'log2', 'W', 'nodes', 'The', 'hierarchical', 'softmax', 'uses', 'a', 'binary', 'tree', 'representation', 'of', 'the', 'output', 'layer', 'with', 'the', 'W', 'words', 'as', 'its', 'leaves', 'and', 'for', 'each', 'node', 'explicitly', 'represents', 'the', 'relative', 'probabilities', 'of', 'its', 'child', 'nodes', 'These', 'deﬁne', 'a', 'random', 'walk', 'that', 'assigns', 'probabilities', 'to', 'words', 'More', 'precisely', 'each', 'word', 'w', 'can', 'be', 'reached', 'by', 'an', 'appropriate', 'path', 'from', 'the', 'root', 'of', 'the', 'tree', 'Let', 'n', 'w', 'j', 'be', 'the', 'j', 'th', 'node', 'on', 'the', 'path', 'from', 'the', 'root', 'to', 'w', 'and', 'let', 'L', 'w', 'be', 'the', 'length', 'of', 'this', 'path', 'so', 'n', 'w', 'root', 'and', 'n', 'w', 'L', 'w', 'w', 'In', 'addition', 'for', 'any', 'inner', 'node', 'n', 'let', 'ch', 'n', 'be', 'an', 'arbitrary', 'ﬁxed', 'child', 'of', 'n', 'and', 'let', 'x', 'be', 'if', 'x', 'is', 'true', 'and', 'otherwise', 'Then', 'the', 'hierarchical', 'softmax', 'deﬁnes', 'p', 'wO', 'wI', 'as', 'follows', 'p', 'w', 'wI', 'L', 'w', 'Y', 'j', 'σ', 'n', 'w', 'j', 'ch', 'n', 'w', 'j', 'v', 'n', 'w', 'j', 'vwI', 'where', 'σ', 'x', 'exp', 'x', 'It', 'can', 'be', 'veriﬁed', 'that', 'PW', 'w', 'p', 'w', 'wI', 'This', 'implies', 'that', 'the', 'cost', 'of', 'computing', 'log', 'p', 'wO', 'wI', 'and', 'log', 'p', 'wO', 'wI', 'is', 'proportional', 'to', 'L', 'wO', 'which', 'on', 'average', 'is', 'no', 'greater', 'than', 'log', 'W', 'Also', 'unlike', 'the', 'standard', 'softmax', 'formulation', 'of', 'the', 'Skip', 'gram', 'which', 'assigns', 'two', 'representations', 'vw', 'and', 'v', 'w', 'to', 'each', 'word', 'w', 'the', 'hierarchical', 'softmax', 'formulation', 'has', 'one', 'representation', 'vw', 'for', 'each', 'word', 'w', 'and', 'one', 'representation', 'v', 'n', 'for', 'every', 'inner', 'node', 'n', 'of', 'the', 'binary', 'tree', 'The', 'structure', 'of', 'the', 'tree', 'used', 'by', 'the', 'hierarchical', 'softmax', 'has', 'a', 'considerable', 'effect', 'on', 'the', 'perfor', 'mance', 'Mnih', 'and', 'Hinton', 'explored', 'a', 'number', 'of', 'methods', 'for', 'constructing', 'the', 'tree', 'structure', 'and', 'the', 'effect', 'on', 'both', 'the', 'training', 'time', 'and', 'the', 'resulting', 'model', 'accuracy', 'In', 'our', 'work', 'we', 'use', 'a', 'binary', 'Huffman', 'tree', 'as', 'it', 'assigns', 'short', 'codes', 'to', 'the', 'frequent', 'words', 'which', 'results', 'in', 'fast', 'training', 'It', 'has', 'been', 'observed', 'before', 'that', 'grouping', 'words', 'together', 'by', 'their', 'frequency', 'works', 'well', 'as', 'a', 'very', 'simple', 'speedup', 'technique', 'for', 'the', 'neural', 'network', 'based', 'language', 'models', 'Negative', 'Sampling', 'An', 'alternative', 'to', 'the', 'hierarchical', 'softmax', 'is', 'Noise', 'Contrastive', 'Estimation', 'NCE', 'which', 'was', 'in', 'troduced', 'by', 'Gutmann', 'and', 'Hyvarinen', 'and', 'applied', 'to', 'language', 'modeling', 'by', 'Mnih', 'and', 'Teh', 'NCE', 'posits', 'that', 'a', 'good', 'model', 'should', 'be', 'able', 'to', 'differentiate', 'data', 'from', 'noise', 'by', 'means', 'of', 'logistic', 'regression', 'This', 'is', 'similar', 'to', 'hinge', 'loss', 'used', 'by', 'Collobert', 'and', 'Weston', 'who', 'trained', 'the', 'models', 'by', 'ranking', 'the', 'data', 'above', 'noise', 'While', 'NCE', 'can', 'be', 'shown', 'to', 'approximately', 'maximize', 'the', 'log', 'probability', 'of', 'the', 'softmax', 'the', 'Skip', 'gram', 'model', 'is', 'only', 'concerned', 'with', 'learning', 'high', 'quality', 'vector', 'representations', 'so', 'we', 'are', 'free', 'to', 'simplify', 'NCE', 'as', 'long', 'as', 'the', 'vector', 'representations', 'retain', 'their', 'quality', 'We', 'deﬁne', 'Negative', 'sampling', 'NEG', 'by', 'the', 'objective', 'log', 'σ', 'v', 'wO', 'vwI', 'k', 'X', 'i', 'Ewi', 'Pn', 'w', 'h', 'log', 'σ', 'v', 'wi', 'vwI', 'i', 'Country', 'and', 'Capital', 'Vectors', 'Projected', 'by', 'PCA', 'China', 'Japan', 'France', 'Russia', 'Germany', 'Italy', 'Spain', 'Greece', 'Turkey', 'Beijing', 'Paris', 'Tokyo', 'Poland', 'Moscow', 'Portugal', 'Berlin', 'Rome', 'Athens', 'Madrid', 'Ankara', 'Warsaw', 'Lisbon', 'Figure', 'Two', 'dimensional', 'PCA', 'projection', 'of', 'the', 'dimensional', 'Skip', 'gram', 'vectors', 'of', 'countries', 'and', 'their', 'capital', 'cities', 'The', 'ﬁgure', 'illustrates', 'ability', 'of', 'the', 'model', 'to', 'automatically', 'organize', 'concepts', 'and', 'learn', 'implicitly', 'the', 'relationships', 'between', 'them', 'as', 'during', 'the', 'training', 'we', 'did', 'not', 'provide', 'any', 'supervised', 'information', 'about', 'what', 'a', 'capital', 'city', 'means', 'which', 'is', 'used', 'to', 'replace', 'every', 'log', 'P', 'wO', 'wI', 'term', 'in', 'the', 'Skip', 'gram', 'objective', 'Thus', 'the', 'task', 'is', 'to', 'distinguish', 'the', 'target', 'word', 'wO', 'from', 'draws', 'from', 'the', 'noise', 'distribution', 'Pn', 'w', 'using', 'logistic', 'regres', 'sion', 'where', 'there', 'are', 'k', 'negative', 'samples', 'for', 'each', 'data', 'sample', 'Our', 'experiments', 'indicate', 'that', 'values', 'of', 'k', 'in', 'the', 'range', 'are', 'useful', 'for', 'small', 'training', 'datasets', 'while', 'for', 'large', 'datasets', 'the', 'k', 'can', 'be', 'as', 'small', 'as', 'The', 'main', 'difference', 'between', 'the', 'Negative', 'sampling', 'and', 'NCE', 'is', 'that', 'NCE', 'needs', 'both', 'samples', 'and', 'the', 'numerical', 'probabilities', 'of', 'the', 'noise', 'distribution', 'while', 'Negative', 'sampling', 'uses', 'only', 'samples', 'And', 'while', 'NCE', 'approximately', 'maximizes', 'the', 'log', 'probability', 'of', 'the', 'softmax', 'this', 'property', 'is', 'not', 'important', 'for', 'our', 'application', 'Both', 'NCE', 'and', 'NEG', 'have', 'the', 'noise', 'distribution', 'Pn', 'w', 'as', 'a', 'free', 'parameter', 'We', 'investigated', 'a', 'number', 'of', 'choices', 'for', 'Pn', 'w', 'and', 'found', 'that', 'the', 'unigram', 'distribution', 'U', 'w', 'raised', 'to', 'the', '4rd', 'power', 'i', 'e', 'U', 'w', 'Z', 'outperformed', 'signiﬁcantly', 'the', 'unigram', 'and', 'the', 'uniform', 'distributions', 'for', 'both', 'NCE', 'and', 'NEG', 'on', 'every', 'task', 'we', 'tried', 'including', 'language', 'modeling', 'not', 'reported', 'here', 'Subsampling', 'of', 'Frequent', 'Words', 'In', 'very', 'large', 'corpora', 'the', 'most', 'frequent', 'words', 'can', 'easily', 'occur', 'hundreds', 'of', 'millions', 'of', 'times', 'e', 'g', 'in', 'the', 'and', 'a', 'Such', 'words', 'usually', 'provide', 'less', 'information', 'value', 'than', 'the', 'rare', 'words', 'For', 'example', 'while', 'the', 'Skip', 'gram', 'model', 'beneﬁts', 'from', 'observing', 'the', 'co', 'occurrences', 'of', 'France', 'and', 'Paris', 'it', 'beneﬁts', 'much', 'less', 'from', 'observing', 'the', 'frequent', 'co', 'occurrences', 'of', 'France', 'and', 'the', 'as', 'nearly', 'every', 'word', 'co', 'occurs', 'frequently', 'within', 'a', 'sentence', 'with', 'the', 'This', 'idea', 'can', 'also', 'be', 'applied', 'in', 'the', 'opposite', 'direction', 'the', 'vector', 'representations', 'of', 'frequent', 'words', 'do', 'not', 'change', 'signiﬁcantly', 'after', 'training', 'on', 'several', 'million', 'examples', 'To', 'counter', 'the', 'imbalance', 'between', 'the', 'rare', 'and', 'frequent', 'words', 'we', 'used', 'a', 'simple', 'subsampling', 'ap', 'proach', 'each', 'word', 'wi', 'in', 'the', 'training', 'set', 'is', 'discarded', 'with', 'probability', 'computed', 'by', 'the', 'formula', 'P', 'wi', 's', 't', 'f', 'wi', 'Method', 'Time', 'min', 'Syntactic', 'Semantic', 'Total', 'accuracy', 'NEG', 'NEG', 'HS', 'Huffman', 'NCE', 'The', 'following', 'results', 'use', 'subsampling', 'NEG', 'NEG', 'HS', 'Huffman', 'Table', 'Accuracy', 'of', 'various', 'Skip', 'gram', 'dimensional', 'models', 'on', 'the', 'analogical', 'reasoning', 'task', 'as', 'deﬁned', 'in', 'NEG', 'k', 'stands', 'for', 'Negative', 'Sampling', 'with', 'k', 'negative', 'samples', 'for', 'each', 'positive', 'sample', 'NCE', 'stands', 'for', 'Noise', 'Contrastive', 'Estimation', 'and', 'HS', 'Huffman', 'stands', 'for', 'the', 'Hierarchical', 'Softmax', 'with', 'the', 'frequency', 'based', 'Huffman', 'codes', 'where', 'f', 'wi', 'is', 'the', 'frequency', 'of', 'word', 'wi', 'and', 't', 'is', 'a', 'chosen', 'threshold', 'typically', 'around', 'We', 'chose', 'this', 'subsampling', 'formula', 'because', 'it', 'aggressively', 'subsamples', 'words', 'whose', 'frequency', 'is', 'greater', 'than', 't', 'while', 'preserving', 'the', 'ranking', 'of', 'the', 'frequencies', 'Although', 'this', 'subsampling', 'for', 'mula', 'was', 'chosen', 'heuristically', 'we', 'found', 'it', 'to', 'work', 'well', 'in', 'practice', 'It', 'accelerates', 'learning', 'and', 'even', 'signiﬁcantly', 'improves', 'the', 'accuracy', 'of', 'the', 'learned', 'vectors', 'of', 'the', 'rare', 'words', 'as', 'will', 'be', 'shown', 'in', 'the', 'following', 'sections', 'Empirical', 'Results', 'In', 'this', 'section', 'we', 'evaluate', 'the', 'Hierarchical', 'Softmax', 'HS', 'Noise', 'Contrastive', 'Estimation', 'Negative', 'Sampling', 'and', 'subsampling', 'of', 'the', 'training', 'words', 'We', 'used', 'the', 'analogical', 'reasoning', 'task1', 'introduced', 'by', 'Mikolov', 'et', 'al', 'The', 'task', 'consists', 'of', 'analogies', 'such', 'as', 'Germany', 'Berlin', 'France', 'which', 'are', 'solved', 'by', 'ﬁnding', 'a', 'vector', 'x', 'such', 'that', 'vec', 'x', 'is', 'closest', 'to', 'vec', 'Berlin', 'vec', 'Germany', 'vec', 'France', 'according', 'to', 'the', 'cosine', 'distance', 'we', 'discard', 'the', 'input', 'words', 'from', 'the', 'search', 'This', 'speciﬁc', 'example', 'is', 'considered', 'to', 'have', 'been', 'answered', 'correctly', 'if', 'x', 'is', 'Paris', 'The', 'task', 'has', 'two', 'broad', 'categories', 'the', 'syntactic', 'analogies', 'such', 'as', 'quick', 'quickly', 'slow', 'slowly', 'and', 'the', 'semantic', 'analogies', 'such', 'as', 'the', 'country', 'to', 'capital', 'city', 'relationship', 'For', 'training', 'the', 'Skip', 'gram', 'models', 'we', 'have', 'used', 'a', 'large', 'dataset', 'consisting', 'of', 'various', 'news', 'articles', 'an', 'internal', 'Google', 'dataset', 'with', 'one', 'billion', 'words', 'We', 'discarded', 'from', 'the', 'vocabulary', 'all', 'words', 'that', 'occurred', 'less', 'than', 'times', 'in', 'the', 'training', 'data', 'which', 'resulted', 'in', 'a', 'vocabulary', 'of', 'size', '692K', 'The', 'performance', 'of', 'various', 'Skip', 'gram', 'models', 'on', 'the', 'word', 'analogy', 'test', 'set', 'is', 'reported', 'in', 'Table', 'The', 'table', 'shows', 'that', 'Negative', 'Sampling', 'outperforms', 'the', 'Hierarchical', 'Softmax', 'on', 'the', 'analogical', 'reasoning', 'task', 'and', 'has', 'even', 'slightly', 'better', 'performance', 'than', 'the', 'Noise', 'Contrastive', 'Estimation', 'The', 'subsampling', 'of', 'the', 'frequent', 'words', 'improves', 'the', 'training', 'speed', 'several', 'times', 'and', 'makes', 'the', 'word', 'representations', 'signiﬁcantly', 'more', 'accurate', 'It', 'can', 'be', 'argued', 'that', 'the', 'linearity', 'of', 'the', 'skip', 'gram', 'model', 'makes', 'its', 'vectors', 'more', 'suitable', 'for', 'such', 'linear', 'analogical', 'reasoning', 'but', 'the', 'results', 'of', 'Mikolov', 'et', 'al', 'also', 'show', 'that', 'the', 'vectors', 'learned', 'by', 'the', 'standard', 'sigmoidal', 'recurrent', 'neural', 'networks', 'which', 'are', 'highly', 'non', 'linear', 'improve', 'on', 'this', 'task', 'signiﬁcantly', 'as', 'the', 'amount', 'of', 'the', 'training', 'data', 'increases', 'suggesting', 'that', 'non', 'linear', 'models', 'also', 'have', 'a', 'preference', 'for', 'a', 'linear', 'structure', 'of', 'the', 'word', 'representations', 'Learning', 'Phrases', 'As', 'discussed', 'earlier', 'many', 'phrases', 'have', 'a', 'meaning', 'that', 'is', 'not', 'a', 'simple', 'composition', 'of', 'the', 'mean', 'ings', 'of', 'its', 'individual', 'words', 'To', 'learn', 'vector', 'representation', 'for', 'phrases', 'we', 'ﬁrst', 'ﬁnd', 'words', 'that', 'appear', 'frequently', 'together', 'and', 'infrequently', 'in', 'other', 'contexts', 'For', 'example', 'New', 'York', 'Times', 'and', 'Toronto', 'Maple', 'Leafs', 'are', 'replaced', 'by', 'unique', 'tokens', 'in', 'the', 'training', 'data', 'while', 'a', 'bigram', 'this', 'is', 'will', 'remain', 'unchanged', '1code', 'google', 'com', 'p', 'word2vec', 'source', 'browse', 'trunk', 'questions', 'words', 'txt', 'Newspapers', 'New', 'York', 'New', 'York', 'Times', 'Baltimore', 'Baltimore', 'Sun', 'San', 'Jose', 'San', 'Jose', 'Mercury', 'News', 'Cincinnati', 'Cincinnati', 'Enquirer', 'NHL', 'Teams', 'Boston', 'Boston', 'Bruins', 'Montreal', 'Montreal', 'Canadiens', 'Phoenix', 'Phoenix', 'Coyotes', 'Nashville', 'Nashville', 'Predators', 'NBA', 'Teams', 'Detroit', 'Detroit', 'Pistons', 'Toronto', 'Toronto', 'Raptors', 'Oakland', 'Golden', 'State', 'Warriors', 'Memphis', 'Memphis', 'Grizzlies', 'Airlines', 'Austria', 'Austrian', 'Airlines', 'Spain', 'Spainair', 'Belgium', 'Brussels', 'Airlines', 'Greece', 'Aegean', 'Airlines', 'Company', 'executives', 'Steve', 'Ballmer', 'Microsoft', 'Larry', 'Page', 'Google', 'Samuel', 'J', 'Palmisano', 'IBM', 'Werner', 'Vogels', 'Amazon', 'Table', 'Examples', 'of', 'the', 'analogical', 'reasoning', 'task', 'for', 'phrases', 'the', 'full', 'test', 'set', 'has', 'examples', 'The', 'goal', 'is', 'to', 'compute', 'the', 'fourth', 'phrase', 'using', 'the', 'ﬁrst', 'three', 'Our', 'best', 'model', 'achieved', 'an', 'accuracy', 'of', 'on', 'this', 'dataset', 'This', 'way', 'we', 'can', 'form', 'many', 'reasonable', 'phrases', 'without', 'greatly', 'increasing', 'the', 'size', 'of', 'the', 'vocabu', 'lary', 'in', 'theory', 'we', 'can', 'train', 'the', 'Skip', 'gram', 'model', 'using', 'all', 'n', 'grams', 'but', 'that', 'would', 'be', 'too', 'memory', 'intensive', 'Many', 'techniques', 'have', 'been', 'previously', 'developed', 'to', 'identify', 'phrases', 'in', 'the', 'text', 'however', 'it', 'is', 'out', 'of', 'scope', 'of', 'our', 'work', 'to', 'compare', 'them', 'We', 'decided', 'to', 'use', 'a', 'simple', 'data', 'driven', 'approach', 'where', 'phrases', 'are', 'formed', 'based', 'on', 'the', 'unigram', 'and', 'bigram', 'counts', 'using', 'score', 'wi', 'wj', 'count', 'wiwj', 'δ', 'count', 'wi', 'count', 'wj', 'The', 'δ', 'is', 'used', 'as', 'a', 'discounting', 'coefﬁcient', 'and', 'prevents', 'too', 'many', 'phrases', 'consisting', 'of', 'very', 'infre', 'quent', 'words', 'to', 'be', 'formed', 'The', 'bigrams', 'with', 'score', 'above', 'the', 'chosen', 'threshold', 'are', 'then', 'used', 'as', 'phrases', 'Typically', 'we', 'run', 'passes', 'over', 'the', 'training', 'data', 'with', 'decreasing', 'threshold', 'value', 'allow', 'ing', 'longer', 'phrases', 'that', 'consists', 'of', 'several', 'words', 'to', 'be', 'formed', 'We', 'evaluate', 'the', 'quality', 'of', 'the', 'phrase', 'representations', 'using', 'a', 'new', 'analogical', 'reasoning', 'task', 'that', 'involves', 'phrases', 'Table', 'shows', 'examples', 'of', 'the', 'ﬁve', 'categories', 'of', 'analogies', 'used', 'in', 'this', 'task', 'This', 'dataset', 'is', 'publicly', 'available', 'on', 'the', 'web2', 'Phrase', 'Skip', 'Gram', 'Results', 'Starting', 'with', 'the', 'same', 'news', 'data', 'as', 'in', 'the', 'previous', 'experiments', 'we', 'ﬁrst', 'constructed', 'the', 'phrase', 'based', 'training', 'corpus', 'and', 'then', 'we', 'trained', 'several', 'Skip', 'gram', 'models', 'using', 'different', 'hyper', 'parameters', 'As', 'before', 'we', 'used', 'vector', 'dimensionality', 'and', 'context', 'size', 'This', 'setting', 'already', 'achieves', 'good', 'performance', 'on', 'the', 'phrase', 'dataset', 'and', 'allowed', 'us', 'to', 'quickly', 'compare', 'the', 'Negative', 'Sampling', 'and', 'the', 'Hierarchical', 'Softmax', 'both', 'with', 'and', 'without', 'subsampling', 'of', 'the', 'frequent', 'tokens', 'The', 'results', 'are', 'summarized', 'in', 'Table', 'The', 'results', 'show', 'that', 'while', 'Negative', 'Sampling', 'achieves', 'a', 'respectable', 'accuracy', 'even', 'with', 'k', 'using', 'k', 'achieves', 'considerably', 'better', 'performance', 'Surprisingly', 'while', 'we', 'found', 'the', 'Hierar', 'chical', 'Softmax', 'to', 'achieve', 'lower', 'performance', 'when', 'trained', 'without', 'subsampling', 'it', 'became', 'the', 'best', 'performing', 'method', 'when', 'we', 'downsampled', 'the', 'frequent', 'words', 'This', 'shows', 'that', 'the', 'subsampling', 'can', 'result', 'in', 'faster', 'training', 'and', 'can', 'also', 'improve', 'accuracy', 'at', 'least', 'in', 'some', 'cases', '2code', 'google', 'com', 'p', 'word2vec', 'source', 'browse', 'trunk', 'questions', 'phrases', 'txt', 'Method', 'Dimensionality', 'No', 'subsampling', 'subsampling', 'NEG', 'NEG', 'HS', 'Huffman', 'Table', 'Accuracies', 'of', 'the', 'Skip', 'gram', 'models', 'on', 'the', 'phrase', 'analogy', 'dataset', 'The', 'models', 'were', 'trained', 'on', 'approximately', 'one', 'billion', 'words', 'from', 'the', 'news', 'dataset', 'NEG', 'with', 'subsampling', 'HS', 'with', 'subsampling', 'Vasco', 'de', 'Gama', 'Lingsugur', 'Italian', 'explorer', 'Lake', 'Baikal', 'Great', 'Rift', 'Valley', 'Aral', 'Sea', 'Alan', 'Bean', 'Rebbeca', 'Naomi', 'moonwalker', 'Ionian', 'Sea', 'Ruegen', 'Ionian', 'Islands', 'chess', 'master', 'chess', 'grandmaster', 'Garry', 'Kasparov', 'Table', 'Examples', 'of', 'the', 'closest', 'entities', 'to', 'the', 'given', 'short', 'phrases', 'using', 'two', 'different', 'models', 'Czech', 'currency', 'Vietnam', 'capital', 'German', 'airlines', 'Russian', 'river', 'French', 'actress', 'koruna', 'Hanoi', 'airline', 'Lufthansa', 'Moscow', 'Juliette', 'Binoche', 'Check', 'crown', 'Ho', 'Chi', 'Minh', 'City', 'carrier', 'Lufthansa', 'Volga', 'River', 'Vanessa', 'Paradis', 'Polish', 'zolty', 'Viet', 'Nam', 'ﬂag', 'carrier', 'Lufthansa', 'upriver', 'Charlotte', 'Gainsbourg', 'CTK', 'Vietnamese', 'Lufthansa', 'Russia', 'Cecile', 'De', 'Table', 'Vector', 'compositionality', 'using', 'element', 'wise', 'addition', 'Four', 'closest', 'tokens', 'to', 'the', 'sum', 'of', 'two', 'vectors', 'are', 'shown', 'using', 'the', 'best', 'Skip', 'gram', 'model', 'To', 'maximize', 'the', 'accuracy', 'on', 'the', 'phrase', 'analogy', 'task', 'we', 'increased', 'the', 'amount', 'of', 'the', 'training', 'data', 'by', 'using', 'a', 'dataset', 'with', 'about', 'billion', 'words', 'We', 'used', 'the', 'hierarchical', 'softmax', 'dimensionality', 'of', 'and', 'the', 'entire', 'sentence', 'for', 'the', 'context', 'This', 'resulted', 'in', 'a', 'model', 'that', 'reached', 'an', 'accuracy', 'of', 'We', 'achieved', 'lower', 'accuracy', 'when', 'we', 'reduced', 'the', 'size', 'of', 'the', 'training', 'dataset', 'to', '6B', 'words', 'which', 'suggests', 'that', 'the', 'large', 'amount', 'of', 'the', 'training', 'data', 'is', 'crucial', 'To', 'gain', 'further', 'insight', 'into', 'how', 'different', 'the', 'representations', 'learned', 'by', 'different', 'models', 'are', 'we', 'did', 'inspect', 'manually', 'the', 'nearest', 'neighbours', 'of', 'infrequent', 'phrases', 'using', 'various', 'models', 'In', 'Table', 'we', 'show', 'a', 'sample', 'of', 'such', 'comparison', 'Consistently', 'with', 'the', 'previous', 'results', 'it', 'seems', 'that', 'the', 'best', 'representations', 'of', 'phrases', 'are', 'learned', 'by', 'a', 'model', 'with', 'the', 'hierarchical', 'softmax', 'and', 'subsampling', 'Additive', 'Compositionality', 'We', 'demonstrated', 'that', 'the', 'word', 'and', 'phrase', 'representations', 'learned', 'by', 'the', 'Skip', 'gram', 'model', 'exhibit', 'a', 'linear', 'structure', 'that', 'makes', 'it', 'possible', 'to', 'perform', 'precise', 'analogical', 'reasoning', 'using', 'simple', 'vector', 'arithmetics', 'Interestingly', 'we', 'found', 'that', 'the', 'Skip', 'gram', 'representations', 'exhibit', 'another', 'kind', 'of', 'linear', 'structure', 'that', 'makes', 'it', 'possible', 'to', 'meaningfully', 'combine', 'words', 'by', 'an', 'element', 'wise', 'addition', 'of', 'their', 'vector', 'representations', 'This', 'phenomenon', 'is', 'illustrated', 'in', 'Table', 'The', 'additive', 'property', 'of', 'the', 'vectors', 'can', 'be', 'explained', 'by', 'inspecting', 'the', 'training', 'objective', 'The', 'word', 'vectors', 'are', 'in', 'a', 'linear', 'relationship', 'with', 'the', 'inputs', 'to', 'the', 'softmax', 'nonlinearity', 'As', 'the', 'word', 'vectors', 'are', 'trained', 'to', 'predict', 'the', 'surrounding', 'words', 'in', 'the', 'sentence', 'the', 'vectors', 'can', 'be', 'seen', 'as', 'representing', 'the', 'distribution', 'of', 'the', 'context', 'in', 'which', 'a', 'word', 'appears', 'These', 'values', 'are', 'related', 'logarithmically', 'to', 'the', 'probabilities', 'computed', 'by', 'the', 'output', 'layer', 'so', 'the', 'sum', 'of', 'two', 'word', 'vectors', 'is', 'related', 'to', 'the', 'product', 'of', 'the', 'two', 'context', 'distributions', 'The', 'product', 'works', 'here', 'as', 'the', 'AND', 'function', 'words', 'that', 'are', 'assigned', 'high', 'probabilities', 'by', 'both', 'word', 'vectors', 'will', 'have', 'high', 'probability', 'and', 'the', 'other', 'words', 'will', 'have', 'low', 'probability', 'Thus', 'if', 'Volga', 'River', 'appears', 'frequently', 'in', 'the', 'same', 'sentence', 'together', 'with', 'the', 'words', 'Russian', 'and', 'river', 'the', 'sum', 'of', 'these', 'two', 'word', 'vectors', 'will', 'result', 'in', 'such', 'a', 'feature', 'vector', 'that', 'is', 'close', 'to', 'the', 'vector', 'of', 'Volga', 'River', 'Comparison', 'to', 'Published', 'Word', 'Representations', 'Many', 'authors', 'who', 'previously', 'worked', 'on', 'the', 'neural', 'network', 'based', 'representations', 'of', 'words', 'have', 'published', 'their', 'resulting', 'models', 'for', 'further', 'use', 'and', 'comparison', 'amongst', 'the', 'most', 'well', 'known', 'au', 'thors', 'are', 'Collobert', 'and', 'Weston', 'Turian', 'et', 'al', 'and', 'Mnih', 'and', 'Hinton', 'We', 'downloaded', 'their', 'word', 'vectors', 'from', 'the', 'web3', 'Mikolov', 'et', 'al', 'have', 'already', 'evaluated', 'these', 'word', 'representa', 'tions', 'on', 'the', 'word', 'analogy', 'task', 'where', 'the', 'Skip', 'gram', 'models', 'achieved', 'the', 'best', 'performance', 'with', 'a', 'huge', 'margin', '3http', 'metaoptimize', 'com', 'projects', 'wordreprs', 'Model', 'Redmond', 'Havel', 'ninjutsu', 'grafﬁti', 'capitulate', 'training', 'time', 'Collobert', '50d', 'conyers', 'plauen', 'reiki', 'cheesecake', 'abdicate', 'months', 'lubbock', 'dzerzhinsky', 'kohona', 'gossip', 'accede', 'keene', 'osterreich', 'karate', 'dioramas', 'rearm', 'Turian', '200d', 'McCarthy', 'Jewell', 'gunﬁre', 'few', 'weeks', 'Alston', 'Arzu', 'emotion', 'Cousins', 'Ovitz', 'impunity', 'Mnih', '100d', 'Podhurst', 'Pontiff', 'anaesthetics', 'Mavericks', 'days', 'Harlang', 'Pinochet', 'monkeys', 'planning', 'Agarwal', 'Rodionov', 'Jews', 'hesitated', 'Skip', 'Phrase', 'Redmond', 'Wash', 'Vaclav', 'Havel', 'ninja', 'spray', 'paint', 'capitulation', '1000d', 'day', 'Redmond', 'Washington', 'president', 'Vaclav', 'Havel', 'martial', 'arts', 'graﬁtti', 'capitulated', 'Microsoft', 'Velvet', 'Revolution', 'swordsmanship', 'taggers', 'capitulating', 'Table', 'Examples', 'of', 'the', 'closest', 'tokens', 'given', 'various', 'well', 'known', 'models', 'and', 'the', 'Skip', 'gram', 'model', 'trained', 'on', 'phrases', 'using', 'over', 'billion', 'training', 'words', 'An', 'empty', 'cell', 'means', 'that', 'the', 'word', 'was', 'not', 'in', 'the', 'vocabulary', 'To', 'give', 'more', 'insight', 'into', 'the', 'difference', 'of', 'the', 'quality', 'of', 'the', 'learned', 'vectors', 'we', 'provide', 'empirical', 'comparison', 'by', 'showing', 'the', 'nearest', 'neighbours', 'of', 'infrequent', 'words', 'in', 'Table', 'These', 'examples', 'show', 'that', 'the', 'big', 'Skip', 'gram', 'model', 'trained', 'on', 'a', 'large', 'corpus', 'visibly', 'outperforms', 'all', 'the', 'other', 'models', 'in', 'the', 'quality', 'of', 'the', 'learned', 'representations', 'This', 'can', 'be', 'attributed', 'in', 'part', 'to', 'the', 'fact', 'that', 'this', 'model', 'has', 'been', 'trained', 'on', 'about', 'billion', 'words', 'which', 'is', 'about', 'two', 'to', 'three', 'orders', 'of', 'magnitude', 'more', 'data', 'than', 'the', 'typical', 'size', 'used', 'in', 'the', 'prior', 'work', 'Interestingly', 'although', 'the', 'training', 'set', 'is', 'much', 'larger', 'the', 'training', 'time', 'of', 'the', 'Skip', 'gram', 'model', 'is', 'just', 'a', 'fraction', 'of', 'the', 'time', 'complexity', 'required', 'by', 'the', 'previous', 'model', 'architectures', 'Conclusion', 'This', 'work', 'has', 'several', 'key', 'contributions', 'We', 'show', 'how', 'to', 'train', 'distributed', 'representations', 'of', 'words', 'and', 'phrases', 'with', 'the', 'Skip', 'gram', 'model', 'and', 'demonstrate', 'that', 'these', 'representations', 'exhibit', 'linear', 'structure', 'that', 'makes', 'precise', 'analogical', 'reasoning', 'possible', 'The', 'techniques', 'introduced', 'in', 'this', 'paper', 'can', 'be', 'used', 'also', 'for', 'training', 'the', 'continuous', 'bag', 'of', 'words', 'model', 'introduced', 'in', 'We', 'successfully', 'trained', 'models', 'on', 'several', 'orders', 'of', 'magnitude', 'more', 'data', 'than', 'the', 'previously', 'pub', 'lished', 'models', 'thanks', 'to', 'the', 'computationally', 'efﬁcient', 'model', 'architecture', 'This', 'results', 'in', 'a', 'great', 'improvement', 'in', 'the', 'quality', 'of', 'the', 'learned', 'word', 'and', 'phrase', 'representations', 'especially', 'for', 'the', 'rare', 'entities', 'We', 'also', 'found', 'that', 'the', 'subsampling', 'of', 'the', 'frequent', 'words', 'results', 'in', 'both', 'faster', 'training', 'and', 'signiﬁcantly', 'better', 'representations', 'of', 'uncommon', 'words', 'Another', 'contribution', 'of', 'our', 'paper', 'is', 'the', 'Negative', 'sampling', 'algorithm', 'which', 'is', 'an', 'extremely', 'simple', 'training', 'method', 'that', 'learns', 'accurate', 'representations', 'especially', 'for', 'frequent', 'words', 'The', 'choice', 'of', 'the', 'training', 'algorithm', 'and', 'the', 'hyper', 'parameter', 'selection', 'is', 'a', 'task', 'speciﬁc', 'decision', 'as', 'we', 'found', 'that', 'different', 'problems', 'have', 'different', 'optimal', 'hyperparameter', 'conﬁgurations', 'In', 'our', 'experiments', 'the', 'most', 'crucial', 'decisions', 'that', 'affect', 'the', 'performance', 'are', 'the', 'choice', 'of', 'the', 'model', 'architecture', 'the', 'size', 'of', 'the', 'vectors', 'the', 'subsampling', 'rate', 'and', 'the', 'size', 'of', 'the', 'training', 'window', 'A', 'very', 'interesting', 'result', 'of', 'this', 'work', 'is', 'that', 'the', 'word', 'vectors', 'can', 'be', 'somewhat', 'meaningfully', 'com', 'bined', 'using', 'just', 'simple', 'vector', 'addition', 'Another', 'approach', 'for', 'learning', 'representations', 'of', 'phrases', 'presented', 'in', 'this', 'paper', 'is', 'to', 'simply', 'represent', 'the', 'phrases', 'with', 'a', 'single', 'token', 'Combination', 'of', 'these', 'two', 'approaches', 'gives', 'a', 'powerful', 'yet', 'simple', 'way', 'how', 'to', 'represent', 'longer', 'pieces', 'of', 'text', 'while', 'hav', 'ing', 'minimal', 'computational', 'complexity', 'Our', 'work', 'can', 'thus', 'be', 'seen', 'as', 'complementary', 'to', 'the', 'existing', 'approach', 'that', 'attempts', 'to', 'represent', 'phrases', 'using', 'recursive', 'matrix', 'vector', 'operations', 'We', 'made', 'the', 'code', 'for', 'training', 'the', 'word', 'and', 'phrase', 'vectors', 'based', 'on', 'the', 'techniques', 'described', 'in', 'this', 'paper', 'available', 'as', 'an', 'open', 'source', 'project4', '4code', 'google', 'com', 'p', 'word2vec', 'References', 'Yoshua', 'Bengio', 'R', 'ejean', 'Ducharme', 'Pascal', 'Vincent', 'and', 'Christian', 'Janvin', 'A', 'neural', 'probabilistic', 'language', 'model', 'The', 'Journal', 'of', 'Machine', 'Learning', 'Research', 'Ronan', 'Collobert', 'and', 'Jason', 'Weston', 'A', 'uniﬁed', 'architecture', 'for', 'natural', 'language', 'processing', 'deep', 'neu', 'ral', 'networks', 'with', 'multitask', 'learning', 'In', 'Proceedings', 'of', 'the', '25th', 'international', 'conference', 'on', 'Machine', 'learning', 'pages', 'ACM', 'Xavier', 'Glorot', 'Antoine', 'Bordes', 'and', 'Yoshua', 'Bengio', 'Domain', 'adaptation', 'for', 'large', 'scale', 'sentiment', 'classi', 'ﬁcation', 'A', 'deep', 'learning', 'approach', 'In', 'ICML', 'Michael', 'U', 'Gutmann', 'and', 'Aapo', 'Hyv', 'arinen', 'Noise', 'contrastive', 'estimation', 'of', 'unnormalized', 'statistical', 'mod', 'els', 'with', 'applications', 'to', 'natural', 'image', 'statistics', 'The', 'Journal', 'of', 'Machine', 'Learning', 'Research', 'Tomas', 'Mikolov', 'Stefan', 'Kombrink', 'Lukas', 'Burget', 'Jan', 'Cernocky', 'and', 'Sanjeev', 'Khudanpur', 'Extensions', 'of', 'recurrent', 'neural', 'network', 'language', 'model', 'In', 'Acoustics', 'Speech', 'and', 'Signal', 'Processing', 'ICASSP', 'IEEE', 'International', 'Conference', 'on', 'pages', 'IEEE', 'Tomas', 'Mikolov', 'Anoop', 'Deoras', 'Daniel', 'Povey', 'Lukas', 'Burget', 'and', 'Jan', 'Cernocky', 'Strategies', 'for', 'Training', 'Large', 'Scale', 'Neural', 'Network', 'Language', 'Models', 'In', 'Proc', 'Automatic', 'Speech', 'Recognition', 'and', 'Understand', 'ing', 'Tomas', 'Mikolov', 'Statistical', 'Language', 'Models', 'Based', 'on', 'Neural', 'Networks', 'PhD', 'thesis', 'PhD', 'Thesis', 'Brno', 'University', 'of', 'Technology', 'Tomas', 'Mikolov', 'Kai', 'Chen', 'Greg', 'Corrado', 'and', 'Jeffrey', 'Dean', 'Efﬁcient', 'estimation', 'of', 'word', 'representations', 'in', 'vector', 'space', 'ICLR', 'Workshop', 'Tomas', 'Mikolov', 'Wen', 'tau', 'Yih', 'and', 'Geoffrey', 'Zweig', 'Linguistic', 'Regularities', 'in', 'Continuous', 'Space', 'Word', 'Representations', 'In', 'Proceedings', 'of', 'NAACL', 'HLT', 'Andriy', 'Mnih', 'and', 'Geoffrey', 'E', 'Hinton', 'A', 'scalable', 'hierarchical', 'distributed', 'language', 'model', 'Advances', 'in', 'neural', 'information', 'processing', 'systems', 'Andriy', 'Mnih', 'and', 'Yee', 'Whye', 'Teh', 'A', 'fast', 'and', 'simple', 'algorithm', 'for', 'training', 'neural', 'probabilistic', 'language', 'models', 'arXiv', 'preprint', 'arXiv', 'Frederic', 'Morin', 'and', 'Yoshua', 'Bengio', 'Hierarchical', 'probabilistic', 'neural', 'network', 'language', 'model', 'In', 'Pro', 'ceedings', 'of', 'the', 'international', 'workshop', 'on', 'artiﬁcial', 'intelligence', 'and', 'statistics', 'pages', 'David', 'E', 'Rumelhart', 'Geoffrey', 'E', 'Hintont', 'and', 'Ronald', 'J', 'Williams', 'Learning', 'representations', 'by', 'back', 'propagating', 'errors', 'Nature', 'Holger', 'Schwenk', 'Continuous', 'space', 'language', 'models', 'Computer', 'Speech', 'and', 'Language', 'vol', 'Richard', 'Socher', 'Cliff', 'C', 'Lin', 'Andrew', 'Y', 'Ng', 'and', 'Christopher', 'D', 'Manning', 'Parsing', 'natural', 'scenes', 'and', 'natural', 'language', 'with', 'recursive', 'neural', 'networks', 'In', 'Proceedings', 'of', 'the', '26th', 'International', 'Conference', 'on', 'Machine', 'Learning', 'ICML', 'volume', 'Richard', 'Socher', 'Brody', 'Huval', 'Christopher', 'D', 'Manning', 'and', 'Andrew', 'Y', 'Ng', 'Semantic', 'Compositionality', 'Through', 'Recursive', 'Matrix', 'Vector', 'Spaces', 'In', 'Proceedings', 'of', 'the', 'Conference', 'on', 'Empirical', 'Methods', 'in', 'Natural', 'Language', 'Processing', 'EMNLP', 'Joseph', 'Turian', 'Lev', 'Ratinov', 'and', 'Yoshua', 'Bengio', 'Word', 'representations', 'a', 'simple', 'and', 'general', 'method', 'for', 'semi', 'supervised', 'learning', 'In', 'Proceedings', 'of', 'the', '48th', 'Annual', 'Meeting', 'of', 'the', 'Association', 'for', 'Computa', 'tional', 'Linguistics', 'pages', 'Association', 'for', 'Computational', 'Linguistics', 'Peter', 'D', 'Turney', 'and', 'Patrick', 'Pantel', 'From', 'frequency', 'to', 'meaning', 'Vector', 'space', 'models', 'of', 'semantics', 'In', 'Journal', 'of', 'Artiﬁcial', 'Intelligence', 'Research', 'Peter', 'D', 'Turney', 'Distributional', 'semantics', 'beyond', 'words', 'Supervised', 'learning', 'of', 'analogy', 'and', 'paraphrase', 'In', 'Transactions', 'of', 'the', 'Association', 'for', 'Computational', 'Linguistics', 'TACL', 'Jason', 'Weston', 'Samy', 'Bengio', 'and', 'Nicolas', 'Usunier', 'Wsabie', 'Scaling', 'up', 'to', 'large', 'vocabulary', 'image', 'annota', 'tion', 'In', 'Proceedings', 'of', 'the', 'Twenty', 'Second', 'international', 'joint', 'conference', 'on', 'Artiﬁcial', 'Intelligence', 'Volume', 'Volume', 'Three', 'pages', 'AAAI', 'Press']\n"
     ]
    }
   ],
   "source": [
    "words_dist = re.findall(r'\\b(?!\\d+\\b)\\w+\\b', decoded_content)\n",
    "\n",
    "print(words_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ed463-1129-4700-9c56-664490249288",
   "metadata": {},
   "source": [
    "I would use list of stop words from nltk library and some additions(https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://gist.github.com/sebleier/554280&ved=2ahUKEwi_yr6BisiGAxXehv0HHSQ_Bw0QFnoECBoQAQ&usg=AOvVaw2MnvfyOnFEZMA88TBgjhV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479ce45b-7210-4768-9fa2-c11076a91cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n",
    "              \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',\n",
    "              'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that',\n",
    "              \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', \n",
    "              'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
    "              'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
    "              'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "              'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "              'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    "              'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\",\n",
    "              'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn',\n",
    "              \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\",\n",
    "              \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \n",
    "              \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\",\n",
    "              \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\n",
    "              \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\",\n",
    "              \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\",\n",
    "              \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\",\n",
    "              \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\",\n",
    "              \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\",\n",
    "              \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\",\n",
    "              \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \n",
    "              \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\",\n",
    "              \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\",\n",
    "              \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\",\n",
    "              \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \n",
    "              \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\",\n",
    "              \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \n",
    "              \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\",\n",
    "              \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\",\n",
    "              \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\",\n",
    "              \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \n",
    "              \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \n",
    "              \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\",\n",
    "              \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\",\n",
    "              \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \n",
    "              \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\",\n",
    "              \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\",\n",
    "              \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\",\n",
    "              \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\",\n",
    "              \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\",\n",
    "              \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \n",
    "              \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \n",
    "              \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \n",
    "              \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\",\n",
    "              \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \n",
    "              \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\",\n",
    "              \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\",\n",
    "              \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \n",
    "              \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \n",
    "              \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \n",
    "              \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \n",
    "              \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \n",
    "              \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\",\n",
    "              \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \n",
    "              \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\",\n",
    "              \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \n",
    "              \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\",\n",
    "              \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\",\n",
    "              \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\",\n",
    "              \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\",\n",
    "              \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\",\n",
    "              \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\",\n",
    "              \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\",\n",
    "              \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\",\n",
    "              \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\",\n",
    "              \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\",\n",
    "              \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\",\n",
    "              \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\",\n",
    "              \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\",\n",
    "              \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\",\n",
    "              \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\",\n",
    "              \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \n",
    "              \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \n",
    "              \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\",\n",
    "              \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\",\n",
    "              \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\",\n",
    "              \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\",\n",
    "              \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\",\n",
    "              \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \n",
    "              \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\",\n",
    "              \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \n",
    "              \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\",\n",
    "              \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \n",
    "              \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \n",
    "              \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \n",
    "              \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\",\n",
    "              \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\",\n",
    "              \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\",\n",
    "              \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\",\n",
    "              \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\",\n",
    "              \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\",\n",
    "              \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a27ae-c9ca-4403-8cdc-6e243ca8d437",
   "metadata": {},
   "source": [
    "Remove all stop-words from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a9b328-06d7-40f2-8008-5e8c495e0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dist = [word for word in words_dist if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620defc-6e43-4bd6-8e2f-abc88507d5c7",
   "metadata": {},
   "source": [
    "Do the same steps for the second paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408fcb25-e0aa-4d44-857a-57ea9c7fa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(\"paper2.pdf\") # open a document\n",
    "out = open(\"output1.txt\", \"wb\") # create a text output\n",
    "for page in doc: # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "    out.write(text) # write text of page\n",
    "    out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a88ee04-cbf6-49b1-b323-30df8981c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output1.txt\", \"rb\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "words_att = re.findall(r'\\b(?!\\d+\\b)\\w+\\b', content.decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e213fae8-3474-4035-adfb-d2a9d2124520",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_att = [word for word in words_att if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848798c-74c7-4014-a9a9-382ccc641399",
   "metadata": {},
   "source": [
    "### Step 2:  Implement the Bag of Words model\n",
    "\n",
    "To do this, I add words from both texts to one dictionary, where the key - the word itself and value is the number of times this word is in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c775290-c5a5-47d9-89b3-e4cf6c71893e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Provided': 1,\n",
       " 'proper': 1,\n",
       " 'attribution': 1,\n",
       " 'provided': 1,\n",
       " 'Google': 17,\n",
       " 'grants': 1,\n",
       " 'permission': 1,\n",
       " 'reproduce': 1,\n",
       " 'tables': 1,\n",
       " 'figures': 1,\n",
       " 'paper': 8,\n",
       " 'solely': 2,\n",
       " 'journalistic': 1,\n",
       " 'scholarly': 1,\n",
       " 'works': 3,\n",
       " 'Attention': 18,\n",
       " 'Ashish': 2,\n",
       " 'Vaswani': 1,\n",
       " 'Brain': 4,\n",
       " 'avaswani': 1,\n",
       " 'google': 14,\n",
       " 'Noam': 4,\n",
       " 'Shazeer': 3,\n",
       " 'noam': 1,\n",
       " 'Niki': 2,\n",
       " 'Parmar': 1,\n",
       " 'nikip': 1,\n",
       " 'Jakob': 3,\n",
       " 'Uszkoreit': 2,\n",
       " 'usz': 1,\n",
       " 'Llion': 2,\n",
       " 'Jones': 1,\n",
       " 'llion': 1,\n",
       " 'Aidan': 2,\n",
       " 'Gomez': 1,\n",
       " 'University': 2,\n",
       " 'Toronto': 8,\n",
       " 'aidan': 1,\n",
       " 'toronto': 1,\n",
       " 'Łukasz': 3,\n",
       " 'Kaiser': 7,\n",
       " 'lukaszkaiser': 1,\n",
       " 'Illia': 2,\n",
       " 'Polosukhin': 1,\n",
       " 'illia': 1,\n",
       " 'polosukhin': 1,\n",
       " 'gmail': 1,\n",
       " 'Abstract': 2,\n",
       " 'dominant': 1,\n",
       " 'sequence': 40,\n",
       " 'transduction': 8,\n",
       " 'models': 60,\n",
       " 'based': 15,\n",
       " 'complex': 2,\n",
       " 'recurrent': 17,\n",
       " 'convolutional': 5,\n",
       " 'neural': 28,\n",
       " 'networks': 19,\n",
       " 'include': 1,\n",
       " 'encoder': 25,\n",
       " 'decoder': 24,\n",
       " 'performing': 3,\n",
       " 'connect': 2,\n",
       " 'attention': 75,\n",
       " 'mechanism': 6,\n",
       " 'propose': 2,\n",
       " 'simple': 18,\n",
       " 'network': 16,\n",
       " 'architecture': 10,\n",
       " 'Transformer': 26,\n",
       " 'mechanisms': 6,\n",
       " 'dispensing': 1,\n",
       " 'recurrence': 4,\n",
       " 'convolutions': 6,\n",
       " 'Experiments': 1,\n",
       " 'machine': 16,\n",
       " 'translation': 23,\n",
       " 'tasks': 16,\n",
       " 'superior': 1,\n",
       " 'quality': 15,\n",
       " 'parallelizable': 1,\n",
       " 'requiring': 1,\n",
       " 'time': 11,\n",
       " 'train': 7,\n",
       " 'model': 76,\n",
       " 'achieves': 6,\n",
       " 'BLEU': 11,\n",
       " 'WMT': 8,\n",
       " 'English': 19,\n",
       " 'German': 9,\n",
       " 'task': 23,\n",
       " 'improving': 3,\n",
       " 'existing': 2,\n",
       " 'including': 5,\n",
       " 'ensembles': 4,\n",
       " 'French': 8,\n",
       " 'establishes': 1,\n",
       " 'single': 13,\n",
       " 'state': 10,\n",
       " 'art': 9,\n",
       " 'score': 6,\n",
       " 'training': 65,\n",
       " 'days': 5,\n",
       " 'GPUs': 6,\n",
       " 'small': 7,\n",
       " 'fraction': 4,\n",
       " 'costs': 2,\n",
       " 'literature': 2,\n",
       " 'generalizes': 2,\n",
       " 'applying': 1,\n",
       " 'constituency': 3,\n",
       " 'parsing': 5,\n",
       " 'large': 18,\n",
       " 'limited': 2,\n",
       " 'data': 17,\n",
       " 'Equal': 1,\n",
       " 'contribution': 2,\n",
       " 'Listing': 1,\n",
       " 'order': 5,\n",
       " 'random': 3,\n",
       " 'proposed': 2,\n",
       " 'replacing': 3,\n",
       " 'RNNs': 2,\n",
       " 'started': 1,\n",
       " 'effort': 1,\n",
       " 'evaluate': 8,\n",
       " 'idea': 3,\n",
       " 'designed': 2,\n",
       " 'implemented': 4,\n",
       " 'crucially': 1,\n",
       " 'involved': 3,\n",
       " 'aspect': 1,\n",
       " 'work': 16,\n",
       " 'scaled': 2,\n",
       " 'dot': 11,\n",
       " 'product': 10,\n",
       " 'multi': 7,\n",
       " 'head': 10,\n",
       " 'parameter': 4,\n",
       " 'free': 3,\n",
       " 'position': 13,\n",
       " 'representation': 10,\n",
       " 'person': 1,\n",
       " 'tuned': 1,\n",
       " 'evaluated': 2,\n",
       " 'countless': 2,\n",
       " 'variants': 2,\n",
       " 'original': 2,\n",
       " 'codebase': 3,\n",
       " 'tensor2tensor': 3,\n",
       " 'experimented': 2,\n",
       " 'responsible': 1,\n",
       " 'initial': 1,\n",
       " 'efficient': 2,\n",
       " 'inference': 3,\n",
       " 'visualizations': 1,\n",
       " 'Lukasz': 2,\n",
       " 'spent': 1,\n",
       " 'long': 9,\n",
       " 'designing': 1,\n",
       " 'parts': 1,\n",
       " 'implementing': 1,\n",
       " 'earlier': 2,\n",
       " 'greatly': 2,\n",
       " 'massively': 1,\n",
       " 'accelerating': 1,\n",
       " 'Work': 2,\n",
       " 'performed': 4,\n",
       " '31st': 1,\n",
       " 'Conference': 11,\n",
       " 'Neural': 14,\n",
       " 'Processing': 9,\n",
       " 'Systems': 5,\n",
       " 'NIPS': 2,\n",
       " 'Long': 4,\n",
       " 'Beach': 1,\n",
       " 'USA': 1,\n",
       " 'arXiv': 36,\n",
       " '03762v7': 1,\n",
       " 'Aug': 1,\n",
       " 'Introduction': 2,\n",
       " 'Recurrent': 5,\n",
       " 'short': 5,\n",
       " 'term': 5,\n",
       " 'memory': 9,\n",
       " 'gated': 3,\n",
       " 'firmly': 1,\n",
       " 'established': 1,\n",
       " 'approaches': 3,\n",
       " 'modeling': 10,\n",
       " 'problems': 3,\n",
       " 'language': 22,\n",
       " 'Numerous': 1,\n",
       " 'efforts': 1,\n",
       " 'continued': 1,\n",
       " 'push': 1,\n",
       " 'boundaries': 1,\n",
       " 'architectures': 7,\n",
       " 'typically': 2,\n",
       " 'factor': 4,\n",
       " 'computation': 8,\n",
       " 'symbol': 3,\n",
       " 'positions': 23,\n",
       " 'input': 26,\n",
       " 'output': 33,\n",
       " 'sequences': 5,\n",
       " 'Aligning': 1,\n",
       " 'steps': 5,\n",
       " 'generate': 1,\n",
       " 'hidden': 5,\n",
       " 'states': 1,\n",
       " 'ht': 2,\n",
       " 'function': 17,\n",
       " 'previous': 10,\n",
       " 'inherently': 1,\n",
       " 'sequential': 7,\n",
       " 'nature': 1,\n",
       " 'precludes': 1,\n",
       " 'parallelization': 2,\n",
       " 'examples': 9,\n",
       " 'critical': 1,\n",
       " 'longer': 5,\n",
       " 'lengths': 3,\n",
       " 'constraints': 2,\n",
       " 'limit': 1,\n",
       " 'batching': 1,\n",
       " 'achieved': 4,\n",
       " 'improvements': 1,\n",
       " 'computational': 6,\n",
       " 'efficiency': 1,\n",
       " 'factorization': 1,\n",
       " 'tricks': 2,\n",
       " 'conditional': 1,\n",
       " 'performance': 11,\n",
       " 'case': 5,\n",
       " 'fundamental': 1,\n",
       " 'constraint': 1,\n",
       " 'remains': 1,\n",
       " 'integral': 1,\n",
       " 'compelling': 1,\n",
       " 'transduc': 1,\n",
       " 'tion': 3,\n",
       " 'allowing': 1,\n",
       " 'dependencies': 9,\n",
       " 'regard': 1,\n",
       " 'distance': 4,\n",
       " 'cases': 2,\n",
       " 'conjunction': 1,\n",
       " 'eschewing': 1,\n",
       " 'relying': 2,\n",
       " 'draw': 1,\n",
       " 'global': 1,\n",
       " 'reach': 1,\n",
       " 'trained': 18,\n",
       " 'hours': 2,\n",
       " 'P100': 4,\n",
       " 'Background': 1,\n",
       " 'goal': 2,\n",
       " 'reducing': 2,\n",
       " 'forms': 1,\n",
       " 'foundation': 1,\n",
       " 'Extended': 1,\n",
       " 'GPU': 2,\n",
       " 'ByteNet': 3,\n",
       " 'ConvS2S': 4,\n",
       " 'basic': 3,\n",
       " 'building': 1,\n",
       " 'block': 1,\n",
       " 'computing': 3,\n",
       " 'representations': 47,\n",
       " 'parallel': 4,\n",
       " 'number': 13,\n",
       " 'operations': 9,\n",
       " 'required': 3,\n",
       " 'relate': 1,\n",
       " 'signals': 2,\n",
       " 'arbitrary': 2,\n",
       " 'grows': 1,\n",
       " 'linearly': 3,\n",
       " 'logarithmically': 2,\n",
       " 'difficult': 4,\n",
       " 'learn': 10,\n",
       " 'distant': 2,\n",
       " 'reduced': 4,\n",
       " 'constant': 3,\n",
       " 'albeit': 1,\n",
       " 'cost': 7,\n",
       " 'effective': 1,\n",
       " 'resolution': 2,\n",
       " 'averaging': 4,\n",
       " 'weighted': 2,\n",
       " 'counteract': 2,\n",
       " 'Multi': 6,\n",
       " 'Head': 4,\n",
       " 'called': 2,\n",
       " 'intra': 1,\n",
       " 'relating': 1,\n",
       " 'compute': 6,\n",
       " 'variety': 1,\n",
       " 'reading': 2,\n",
       " 'comprehension': 1,\n",
       " 'abstractive': 2,\n",
       " 'summarization': 2,\n",
       " 'textual': 1,\n",
       " 'entailment': 1,\n",
       " 'learning': 24,\n",
       " 'independent': 2,\n",
       " 'sentence': 11,\n",
       " 'aligned': 2,\n",
       " 'perform': 6,\n",
       " 'question': 1,\n",
       " 'answering': 1,\n",
       " 'knowledge': 1,\n",
       " 'convolution': 3,\n",
       " 'sections': 2,\n",
       " 'motivate': 1,\n",
       " 'discuss': 2,\n",
       " 'advantages': 1,\n",
       " 'Model': 6,\n",
       " 'Architecture': 1,\n",
       " 'competitive': 2,\n",
       " 'structure': 9,\n",
       " 'maps': 1,\n",
       " 'continuous': 3,\n",
       " 'z1': 2,\n",
       " 'zn': 2,\n",
       " 'generates': 1,\n",
       " 'y1': 1,\n",
       " 'ym': 1,\n",
       " 'symbols': 2,\n",
       " 'element': 3,\n",
       " 'step': 4,\n",
       " 'auto': 2,\n",
       " 'regressive': 2,\n",
       " 'consuming': 1,\n",
       " 'generated': 1,\n",
       " 'additional': 1,\n",
       " 'generating': 1,\n",
       " 'Figure': 12,\n",
       " 'stacked': 1,\n",
       " 'point': 4,\n",
       " 'wise': 6,\n",
       " 'fully': 3,\n",
       " 'connected': 3,\n",
       " 'layers': 29,\n",
       " 'left': 2,\n",
       " 'halves': 1,\n",
       " 'Encoder': 2,\n",
       " 'Decoder': 2,\n",
       " 'Stacks': 1,\n",
       " 'composed': 3,\n",
       " 'stack': 5,\n",
       " 'identical': 6,\n",
       " 'layer': 35,\n",
       " 'feed': 4,\n",
       " 'forward': 6,\n",
       " 'employ': 4,\n",
       " 'residual': 5,\n",
       " 'connection': 1,\n",
       " 'normalization': 3,\n",
       " 'LayerNorm': 1,\n",
       " 'Sublayer': 2,\n",
       " 'facilitate': 1,\n",
       " 'connections': 4,\n",
       " 'embedding': 6,\n",
       " 'produce': 2,\n",
       " 'outputs': 4,\n",
       " 'dimension': 9,\n",
       " 'dmodel': 12,\n",
       " 'addition': 9,\n",
       " 'inserts': 1,\n",
       " 'performs': 2,\n",
       " 'modify': 1,\n",
       " 'prevent': 3,\n",
       " 'attending': 1,\n",
       " 'subsequent': 1,\n",
       " 'masking': 2,\n",
       " 'combined': 2,\n",
       " 'fact': 2,\n",
       " 'embeddings': 7,\n",
       " 'offset': 2,\n",
       " 'ensures': 1,\n",
       " 'predictions': 1,\n",
       " 'depend': 1,\n",
       " 'mapping': 2,\n",
       " 'query': 4,\n",
       " 'set': 15,\n",
       " 'key': 7,\n",
       " 'pairs': 5,\n",
       " 'keys': 9,\n",
       " 'values': 20,\n",
       " 'vectors': 27,\n",
       " 'computed': 5,\n",
       " 'sum': 4,\n",
       " 'Scaled': 4,\n",
       " 'Dot': 5,\n",
       " 'Product': 4,\n",
       " 'consists': 5,\n",
       " 'running': 1,\n",
       " 'weight': 2,\n",
       " 'assigned': 2,\n",
       " 'compatibility': 4,\n",
       " 'queries': 7,\n",
       " 'dv': 6,\n",
       " 'products': 4,\n",
       " 'divide': 1,\n",
       " 'apply': 4,\n",
       " 'softmax': 22,\n",
       " 'weights': 2,\n",
       " 'practice': 3,\n",
       " 'simultaneously': 1,\n",
       " 'packed': 2,\n",
       " 'matrix': 6,\n",
       " 'matrices': 2,\n",
       " 'QKT': 1,\n",
       " 'commonly': 3,\n",
       " 'functions': 2,\n",
       " 'additive': 3,\n",
       " 'plicative': 1,\n",
       " 'algorithm': 4,\n",
       " 'scaling': 2,\n",
       " 'Additive': 2,\n",
       " 'computes': 1,\n",
       " 'theoretical': 1,\n",
       " 'complexity': 8,\n",
       " 'faster': 6,\n",
       " 'space': 5,\n",
       " 'highly': 2,\n",
       " 'optimized': 2,\n",
       " 'multiplication': 1,\n",
       " 'code': 3,\n",
       " 'outperforms': 6,\n",
       " 'larger': 4,\n",
       " 'suspect': 1,\n",
       " 'grow': 1,\n",
       " 'magnitude': 3,\n",
       " 'pushing': 1,\n",
       " 'regions': 1,\n",
       " 'extremely': 3,\n",
       " 'gradients': 1,\n",
       " 'scale': 2,\n",
       " 'dimensional': 5,\n",
       " 'beneficial': 2,\n",
       " 'project': 1,\n",
       " 'times': 4,\n",
       " 'learned': 16,\n",
       " 'linear': 16,\n",
       " 'projections': 2,\n",
       " 'dimensions': 2,\n",
       " 'projected': 2,\n",
       " 'versions': 2,\n",
       " 'yielding': 2,\n",
       " '4To': 1,\n",
       " 'illustrate': 1,\n",
       " 'assume': 1,\n",
       " 'components': 2,\n",
       " 'variables': 1,\n",
       " 'variance': 2,\n",
       " 'Pdk': 1,\n",
       " 'qiki': 1,\n",
       " 'concatenated': 1,\n",
       " 'final': 1,\n",
       " 'depicted': 1,\n",
       " 'jointly': 2,\n",
       " 'attend': 6,\n",
       " 'subspaces': 1,\n",
       " 'inhibits': 1,\n",
       " 'MultiHead': 1,\n",
       " 'Concat': 1,\n",
       " 'head1': 1,\n",
       " 'headh': 1,\n",
       " 'headi': 1,\n",
       " 'QW': 1,\n",
       " 'KW': 1,\n",
       " 'Rdmodel': 3,\n",
       " 'Rhdv': 1,\n",
       " 'heads': 11,\n",
       " 'total': 3,\n",
       " 'dimensionality': 6,\n",
       " 'Applications': 1,\n",
       " 'ways': 2,\n",
       " 'mimics': 1,\n",
       " 'typical': 4,\n",
       " 'place': 1,\n",
       " 'leftward': 1,\n",
       " 'flow': 2,\n",
       " 'preserve': 1,\n",
       " 'property': 4,\n",
       " 'implement': 1,\n",
       " 'inside': 1,\n",
       " 'setting': 7,\n",
       " 'correspond': 1,\n",
       " 'illegal': 1,\n",
       " 'Position': 1,\n",
       " 'Feed': 1,\n",
       " 'Forward': 1,\n",
       " 'Networks': 2,\n",
       " 'applied': 4,\n",
       " 'separately': 1,\n",
       " 'identically': 1,\n",
       " 'transformations': 2,\n",
       " 'ReLU': 1,\n",
       " 'activation': 1,\n",
       " 'FFN': 1,\n",
       " 'max': 1,\n",
       " 'xW1': 1,\n",
       " 'W2': 1,\n",
       " 'parameters': 3,\n",
       " 'describing': 1,\n",
       " 'kernel': 3,\n",
       " 'size': 16,\n",
       " 'dff': 2,\n",
       " 'Embeddings': 1,\n",
       " 'Softmax': 7,\n",
       " 'convert': 2,\n",
       " 'tokens': 14,\n",
       " 'usual': 1,\n",
       " 'transfor': 1,\n",
       " 'mation': 1,\n",
       " 'predicted': 1,\n",
       " 'token': 2,\n",
       " 'probabilities': 6,\n",
       " 'share': 1,\n",
       " 'pre': 1,\n",
       " 'transformation': 1,\n",
       " 'multiply': 1,\n",
       " 'Table': 25,\n",
       " 'Maximum': 2,\n",
       " 'path': 7,\n",
       " 'minimum': 2,\n",
       " 'types': 3,\n",
       " 'length': 16,\n",
       " 'neighborhood': 2,\n",
       " 'restricted': 4,\n",
       " 'Layer': 3,\n",
       " 'Type': 1,\n",
       " 'Complexity': 1,\n",
       " 'Sequential': 1,\n",
       " 'Path': 1,\n",
       " 'Length': 1,\n",
       " 'Operations': 1,\n",
       " 'Convolutional': 2,\n",
       " 'logk': 2,\n",
       " 'Positional': 1,\n",
       " 'Encoding': 1,\n",
       " 'inject': 1,\n",
       " 'relative': 3,\n",
       " 'absolute': 1,\n",
       " 'add': 1,\n",
       " 'positional': 9,\n",
       " 'encodings': 4,\n",
       " 'bottoms': 1,\n",
       " 'stacks': 2,\n",
       " 'summed': 1,\n",
       " 'choices': 2,\n",
       " 'fixed': 2,\n",
       " 'sine': 1,\n",
       " 'cosine': 2,\n",
       " 'frequencies': 2,\n",
       " 'pos': 5,\n",
       " '2i': 2,\n",
       " 'sin': 1,\n",
       " '100002i': 2,\n",
       " 'cos': 1,\n",
       " 'encoding': 4,\n",
       " 'corresponds': 2,\n",
       " 'sinusoid': 1,\n",
       " 'wavelengths': 1,\n",
       " 'form': 2,\n",
       " 'geometric': 1,\n",
       " 'progression': 1,\n",
       " '2π': 2,\n",
       " 'chose': 3,\n",
       " 'hypothesized': 1,\n",
       " 'easily': 3,\n",
       " 'PEpos': 2,\n",
       " 'represented': 2,\n",
       " 'produced': 1,\n",
       " 'row': 2,\n",
       " 'sinusoidal': 2,\n",
       " 'version': 1,\n",
       " 'extrapolate': 1,\n",
       " 'encountered': 1,\n",
       " 'compare': 4,\n",
       " 'aspects': 1,\n",
       " 'convolu': 1,\n",
       " 'tional': 3,\n",
       " 'variable': 1,\n",
       " 'equal': 2,\n",
       " 'Motivating': 1,\n",
       " 'desiderata': 1,\n",
       " 'parallelized': 1,\n",
       " 'measured': 1,\n",
       " 'range': 5,\n",
       " 'Learning': 11,\n",
       " 'challenge': 1,\n",
       " 'ability': 2,\n",
       " 'paths': 3,\n",
       " 'backward': 1,\n",
       " 'traverse': 1,\n",
       " 'shorter': 1,\n",
       " 'combination': 3,\n",
       " 'easier': 1,\n",
       " 'maximum': 4,\n",
       " 'connects': 1,\n",
       " 'sequentially': 1,\n",
       " 'executed': 1,\n",
       " 'requires': 2,\n",
       " 'terms': 2,\n",
       " 'smaller': 1,\n",
       " 'translations': 2,\n",
       " 'word': 46,\n",
       " 'piece': 2,\n",
       " 'byte': 3,\n",
       " 'pair': 4,\n",
       " 'improve': 5,\n",
       " 'involving': 2,\n",
       " 'centered': 1,\n",
       " 'respective': 1,\n",
       " 'increase': 1,\n",
       " 'plan': 3,\n",
       " 'investigate': 2,\n",
       " 'approach': 7,\n",
       " 'future': 2,\n",
       " 'width': 1,\n",
       " 'contiguous': 1,\n",
       " 'kernels': 1,\n",
       " 'dilated': 1,\n",
       " 'increasing': 3,\n",
       " 'longest': 1,\n",
       " 'generally': 1,\n",
       " 'expensive': 1,\n",
       " 'Separable': 1,\n",
       " 'decrease': 1,\n",
       " 'considerably': 3,\n",
       " 'separable': 2,\n",
       " 'benefit': 1,\n",
       " 'yield': 1,\n",
       " 'interpretable': 2,\n",
       " 'inspect': 2,\n",
       " 'distributions': 3,\n",
       " 'appendix': 1,\n",
       " 'individual': 4,\n",
       " 'exhibit': 5,\n",
       " 'behavior': 1,\n",
       " 'syntactic': 3,\n",
       " 'semantic': 3,\n",
       " 'sentences': 6,\n",
       " 'Training': 6,\n",
       " 'describes': 1,\n",
       " 'regime': 1,\n",
       " 'Data': 1,\n",
       " 'Batching': 1,\n",
       " 'standard': 3,\n",
       " 'dataset': 11,\n",
       " 'consisting': 4,\n",
       " 'Sentences': 1,\n",
       " 'encoded': 1,\n",
       " 'shared': 1,\n",
       " 'source': 5,\n",
       " 'target': 3,\n",
       " 'vocabulary': 9,\n",
       " '36M': 1,\n",
       " 'split': 1,\n",
       " 'Sentence': 1,\n",
       " 'batched': 1,\n",
       " 'approximate': 1,\n",
       " 'batch': 1,\n",
       " 'contained': 1,\n",
       " 'Hardware': 1,\n",
       " 'Schedule': 1,\n",
       " 'NVIDIA': 1,\n",
       " 'base': 11,\n",
       " 'hyperparameters': 2,\n",
       " 'seconds': 2,\n",
       " 'big': 10,\n",
       " 'table': 2,\n",
       " 'Optimizer': 1,\n",
       " 'Adam': 2,\n",
       " 'optimizer': 1,\n",
       " 'β1': 1,\n",
       " 'β2': 1,\n",
       " 'ϵ': 1,\n",
       " 'varied': 2,\n",
       " 'rate': 5,\n",
       " 'formula': 3,\n",
       " 'lrate': 1,\n",
       " 'min': 2,\n",
       " 'step_num': 2,\n",
       " 'warmup_steps': 3,\n",
       " 'decreasing': 2,\n",
       " 'proportionally': 1,\n",
       " 'inverse': 1,\n",
       " 'square': 1,\n",
       " 'root': 4,\n",
       " 'Regularization': 1,\n",
       " 'regularization': 1,\n",
       " 'scores': 1,\n",
       " 'newstest2014': 1,\n",
       " 'tests': 1,\n",
       " 'Cost': 1,\n",
       " 'FLOPs': 1,\n",
       " 'Deep': 5,\n",
       " 'Att': 2,\n",
       " 'PosUnk': 2,\n",
       " 'GNMT': 2,\n",
       " 'MoE': 1,\n",
       " 'Ensemble': 3,\n",
       " 'Residual': 1,\n",
       " 'Dropout': 2,\n",
       " 'dropout': 5,\n",
       " 'normalized': 1,\n",
       " 'sums': 1,\n",
       " 'Pdrop': 3,\n",
       " 'Label': 1,\n",
       " 'Smoothing': 1,\n",
       " 'employed': 1,\n",
       " 'label': 1,\n",
       " 'smoothing': 1,\n",
       " 'ϵls': 2,\n",
       " 'hurts': 2,\n",
       " 'perplexity': 1,\n",
       " 'learns': 2,\n",
       " 'unsure': 1,\n",
       " 'improves': 4,\n",
       " 'accuracy': 12,\n",
       " 'Machine': 6,\n",
       " 'Translation': 1,\n",
       " 'transformer': 2,\n",
       " 'reported': 5,\n",
       " 'establishing': 1,\n",
       " 'configuration': 1,\n",
       " 'listed': 1,\n",
       " 'surpasses': 1,\n",
       " 'published': 3,\n",
       " 'outperforming': 1,\n",
       " 'checkpoints': 2,\n",
       " 'written': 1,\n",
       " 'minute': 1,\n",
       " 'intervals': 1,\n",
       " 'averaged': 1,\n",
       " 'beam': 5,\n",
       " 'search': 3,\n",
       " 'penalty': 1,\n",
       " 'α': 2,\n",
       " 'chosen': 4,\n",
       " 'experimentation': 1,\n",
       " 'development': 4,\n",
       " 'terminate': 1,\n",
       " 'early': 1,\n",
       " 'summarizes': 1,\n",
       " 'compares': 1,\n",
       " 'estimate': 2,\n",
       " 'floating': 2,\n",
       " 'multiplying': 1,\n",
       " 'sustained': 1,\n",
       " 'precision': 1,\n",
       " 'capacity': 1,\n",
       " 'Variations': 2,\n",
       " 'measuring': 1,\n",
       " 'change': 2,\n",
       " '5We': 1,\n",
       " 'TFLOPS': 1,\n",
       " 'K80': 1,\n",
       " 'K40': 1,\n",
       " 'M40': 1,\n",
       " 'Unlisted': 1,\n",
       " 'metrics': 1,\n",
       " 'newstest2013': 2,\n",
       " 'Listed': 1,\n",
       " 'perplexities': 2,\n",
       " 'wordpiece': 1,\n",
       " 'compared': 2,\n",
       " 'PPL': 1,\n",
       " 'params': 1,\n",
       " 'dev': 2,\n",
       " '100K': 1,\n",
       " 'sinusoids': 1,\n",
       " '300K': 1,\n",
       " 'checkpoint': 1,\n",
       " 'rows': 3,\n",
       " 'vary': 1,\n",
       " 'keeping': 1,\n",
       " 'worse': 1,\n",
       " 'drops': 1,\n",
       " 'observe': 3,\n",
       " 'suggests': 3,\n",
       " 'determining': 1,\n",
       " 'easy': 1,\n",
       " 'sophisticated': 1,\n",
       " 'expected': 1,\n",
       " 'bigger': 1,\n",
       " 'helpful': 1,\n",
       " 'avoiding': 1,\n",
       " 'fitting': 1,\n",
       " 'replace': 3,\n",
       " 'Constituency': 1,\n",
       " 'Parsing': 2,\n",
       " 'generalize': 1,\n",
       " 'experiments': 5,\n",
       " 'presents': 1,\n",
       " 'specific': 2,\n",
       " 'challenges': 1,\n",
       " 'subject': 1,\n",
       " 'strong': 1,\n",
       " 'structural': 1,\n",
       " 'RNN': 2,\n",
       " 'attain': 1,\n",
       " 'regimes': 1,\n",
       " 'Wall': 1,\n",
       " 'Street': 1,\n",
       " 'Journal': 5,\n",
       " 'WSJ': 11,\n",
       " 'portion': 1,\n",
       " 'Penn': 1,\n",
       " 'Treebank': 1,\n",
       " '40K': 2,\n",
       " 'semi': 9,\n",
       " 'supervised': 10,\n",
       " 'high': 6,\n",
       " 'confidence': 1,\n",
       " 'BerkleyParser': 1,\n",
       " 'corpora': 2,\n",
       " '17M': 1,\n",
       " '16K': 1,\n",
       " '32K': 1,\n",
       " 'select': 1,\n",
       " 'rates': 1,\n",
       " 'remained': 1,\n",
       " 'unchanged': 2,\n",
       " 'Parser': 2,\n",
       " 'F1': 1,\n",
       " 'Vinyals': 6,\n",
       " 'discriminative': 5,\n",
       " 'Petrov': 3,\n",
       " 'Zhu': 4,\n",
       " 'Dyer': 3,\n",
       " 'Huang': 2,\n",
       " 'Harper': 2,\n",
       " 'McClosky': 2,\n",
       " 'Luong': 5,\n",
       " 'generative': 1,\n",
       " 'increased': 2,\n",
       " 'lack': 1,\n",
       " 'tuning': 1,\n",
       " 'sur': 1,\n",
       " 'prisingly': 1,\n",
       " 'exception': 1,\n",
       " 'Network': 2,\n",
       " 'Grammar': 2,\n",
       " 'contrast': 1,\n",
       " 'Berkeley': 1,\n",
       " 'Conclusion': 2,\n",
       " 'presented': 2,\n",
       " 'headed': 1,\n",
       " 'achieve': 3,\n",
       " 'excited': 1,\n",
       " 'extend': 1,\n",
       " 'modalities': 1,\n",
       " 'text': 5,\n",
       " 'local': 1,\n",
       " 'efficiently': 1,\n",
       " 'handle': 1,\n",
       " 'inputs': 2,\n",
       " 'images': 1,\n",
       " 'audio': 1,\n",
       " 'video': 1,\n",
       " 'Making': 1,\n",
       " 'generation': 1,\n",
       " 'goals': 1,\n",
       " 'https': 1,\n",
       " 'github': 1,\n",
       " 'tensorflow': 1,\n",
       " 'Acknowledgements': 1,\n",
       " 'grateful': 1,\n",
       " 'Nal': 2,\n",
       " 'Kalchbrenner': 2,\n",
       " 'Stephan': 1,\n",
       " 'Gouws': 1,\n",
       " 'fruitful': 1,\n",
       " 'comments': 1,\n",
       " 'corrections': 1,\n",
       " 'inspiration': 1,\n",
       " 'References': 2,\n",
       " 'Jimmy': 2,\n",
       " 'Lei': 1,\n",
       " 'Jamie': 1,\n",
       " 'Ryan': 1,\n",
       " 'Kiros': 1,\n",
       " 'Geoffrey': 6,\n",
       " 'Hinton': 8,\n",
       " 'preprint': 17,\n",
       " 'Dzmitry': 1,\n",
       " 'Bahdanau': 1,\n",
       " 'Kyunghyun': 3,\n",
       " 'Cho': 3,\n",
       " 'Yoshua': 9,\n",
       " 'Bengio': 12,\n",
       " 'align': 1,\n",
       " 'translate': 1,\n",
       " 'CoRR': 6,\n",
       " 'abs': 6,\n",
       " 'Denny': 1,\n",
       " 'Britz': 1,\n",
       " 'Anna': 1,\n",
       " 'Goldie': 1,\n",
       " 'Minh': 4,\n",
       " 'Thang': 3,\n",
       " 'Quoc': 5,\n",
       " 'Massive': 1,\n",
       " 'exploration': 1,\n",
       " 'Jianpeng': 1,\n",
       " 'Cheng': 1,\n",
       " 'Li': 2,\n",
       " 'Dong': 1,\n",
       " 'Mirella': 1,\n",
       " 'Lapata': 1,\n",
       " 'Bart': 1,\n",
       " 'van': 2,\n",
       " 'Merrienboer': 1,\n",
       " 'Caglar': 1,\n",
       " 'Gulcehre': 1,\n",
       " 'Fethi': 1,\n",
       " 'Bougares': 1,\n",
       " 'Holger': 2,\n",
       " 'Schwenk': 2,\n",
       " 'phrase': 14,\n",
       " 'rnn': 1,\n",
       " 'statistical': 3,\n",
       " 'Francois': 1,\n",
       " 'Chollet': 1,\n",
       " 'Xception': 1,\n",
       " 'depthwise': 1,\n",
       " 'Junyoung': 1,\n",
       " 'Chung': 1,\n",
       " 'Çaglar': 1,\n",
       " 'Gülçehre': 1,\n",
       " 'Empirical': 5,\n",
       " 'evaluation': 1,\n",
       " 'Chris': 1,\n",
       " 'Adhiguna': 1,\n",
       " 'Kuncoro': 1,\n",
       " 'Miguel': 1,\n",
       " 'Ballesteros': 1,\n",
       " 'Noah': 1,\n",
       " 'Smith': 1,\n",
       " 'grammars': 2,\n",
       " 'Proc': 2,\n",
       " 'NAACL': 3,\n",
       " 'Jonas': 1,\n",
       " 'Gehring': 1,\n",
       " 'Michael': 2,\n",
       " 'Auli': 1,\n",
       " 'David': 3,\n",
       " 'Grangier': 1,\n",
       " 'Denis': 1,\n",
       " 'Yarats': 1,\n",
       " 'Yann': 1,\n",
       " 'Dauphin': 1,\n",
       " 'Convolu': 1,\n",
       " '03122v2': 1,\n",
       " 'Alex': 3,\n",
       " 'Graves': 2,\n",
       " 'Generating': 1,\n",
       " 'Kaiming': 1,\n",
       " 'Xiangyu': 1,\n",
       " 'Zhang': 3,\n",
       " 'Shaoqing': 1,\n",
       " 'Ren': 1,\n",
       " 'Jian': 1,\n",
       " 'Sun': 2,\n",
       " 'age': 1,\n",
       " 'recognition': 2,\n",
       " 'Proceedings': 11,\n",
       " 'IEEE': 3,\n",
       " 'Computer': 2,\n",
       " 'Vision': 1,\n",
       " 'Pattern': 1,\n",
       " 'Recognition': 2,\n",
       " 'Sepp': 2,\n",
       " 'Hochreiter': 2,\n",
       " 'Paolo': 1,\n",
       " 'Frasconi': 1,\n",
       " 'Jürgen': 2,\n",
       " 'Schmidhuber': 2,\n",
       " 'Gradient': 1,\n",
       " 'nets': 1,\n",
       " 'difficulty': 1,\n",
       " 'Zhongqiang': 1,\n",
       " 'Mary': 2,\n",
       " 'PCFG': 1,\n",
       " 'latent': 1,\n",
       " 'annotations': 1,\n",
       " 'languages': 1,\n",
       " 'Methods': 3,\n",
       " 'Natural': 3,\n",
       " 'Language': 7,\n",
       " 'ACL': 6,\n",
       " 'August': 2,\n",
       " 'Rafal': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words = {}\n",
    "for word in words_att:\n",
    "    if word in bag_words:\n",
    "        bag_words[word] += 1\n",
    "    else:\n",
    "        bag_words[word] = 1\n",
    "for word in words_dist:\n",
    "    if word in bag_words:\n",
    "        bag_words[word] += 1\n",
    "    else:\n",
    "        bag_words[word] = 1\n",
    "bag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea85faff-f922-4d6c-9f2b-3c1dfc942ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2031"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bag_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ad8f1-6405-46b3-b7ef-09c440386659",
   "metadata": {},
   "source": [
    "I want to shrink list of words to 25% of all words and take top-500 most popular words in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8baa1a-93af-4bf1-9469-b2a0f5b14d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'attention',\n",
       " 'training',\n",
       " 'models',\n",
       " 'representations',\n",
       " 'word',\n",
       " 'sequence',\n",
       " 'arXiv',\n",
       " 'layer',\n",
       " 'output',\n",
       " 'Skip',\n",
       " 'gram',\n",
       " 'layers',\n",
       " 'neural',\n",
       " 'vectors',\n",
       " 'phrases',\n",
       " 'Transformer',\n",
       " 'input',\n",
       " 'encoder',\n",
       " 'Table',\n",
       " 'decoder',\n",
       " 'learning',\n",
       " 'vector',\n",
       " 'translation',\n",
       " 'task',\n",
       " 'positions',\n",
       " 'language',\n",
       " 'softmax',\n",
       " 'values',\n",
       " 'pad',\n",
       " 'networks',\n",
       " 'English',\n",
       " 'Attention',\n",
       " 'simple',\n",
       " 'large',\n",
       " 'trained',\n",
       " 'vec',\n",
       " 'Google',\n",
       " 'recurrent',\n",
       " 'data',\n",
       " 'function',\n",
       " 'preprint',\n",
       " 'subsampling',\n",
       " 'network',\n",
       " 'machine',\n",
       " 'tasks',\n",
       " 'work',\n",
       " 'learned',\n",
       " 'linear',\n",
       " 'size',\n",
       " 'length',\n",
       " 'based',\n",
       " 'quality',\n",
       " 'set',\n",
       " 'google',\n",
       " 'Neural',\n",
       " 'tokens',\n",
       " 'phrase',\n",
       " 'frequent',\n",
       " 'single',\n",
       " 'position',\n",
       " 'number',\n",
       " 'Figure',\n",
       " 'dmodel',\n",
       " 'accuracy',\n",
       " 'Bengio',\n",
       " 'NCE',\n",
       " 'time',\n",
       " 'BLEU',\n",
       " 'dot',\n",
       " 'Conference',\n",
       " 'performance',\n",
       " 'sentence',\n",
       " 'heads',\n",
       " 'Learning',\n",
       " 'dataset',\n",
       " 'base',\n",
       " 'WSJ',\n",
       " 'Proceedings',\n",
       " 'hierarchical',\n",
       " 'log',\n",
       " 'NEG',\n",
       " 'architecture',\n",
       " 'state',\n",
       " 'product',\n",
       " 'head',\n",
       " 'representation',\n",
       " 'modeling',\n",
       " 'previous',\n",
       " 'learn',\n",
       " 'big',\n",
       " 'supervised',\n",
       " 'Input',\n",
       " 'EOS',\n",
       " 'Mikolov',\n",
       " 'Negative',\n",
       " 'German',\n",
       " 'art',\n",
       " 'long',\n",
       " 'Processing',\n",
       " 'memory',\n",
       " 'examples',\n",
       " 'dependencies',\n",
       " 'operations',\n",
       " 'structure',\n",
       " 'dimension',\n",
       " 'addition',\n",
       " 'keys',\n",
       " 'positional',\n",
       " 'vocabulary',\n",
       " 'semi',\n",
       " 'Yoshua',\n",
       " 'application',\n",
       " 'reasoning',\n",
       " 'paper',\n",
       " 'Toronto',\n",
       " 'transduction',\n",
       " 'WMT',\n",
       " 'French',\n",
       " 'evaluate',\n",
       " 'computation',\n",
       " 'complexity',\n",
       " 'Hinton',\n",
       " 'Law',\n",
       " 'perfect',\n",
       " 'missing',\n",
       " 'opinion',\n",
       " 'analogical',\n",
       " 'Kaiser',\n",
       " 'train',\n",
       " 'small',\n",
       " 'multi',\n",
       " 'architectures',\n",
       " 'sequential',\n",
       " 'cost',\n",
       " 'embeddings',\n",
       " 'key',\n",
       " 'queries',\n",
       " 'setting',\n",
       " 'Softmax',\n",
       " 'path',\n",
       " 'approach',\n",
       " 'Language',\n",
       " 'method',\n",
       " 'tree',\n",
       " 'represent',\n",
       " 'probability',\n",
       " 'mechanism',\n",
       " 'mechanisms',\n",
       " 'convolutions',\n",
       " 'achieves',\n",
       " 'score',\n",
       " 'GPUs',\n",
       " 'computational',\n",
       " 'Multi',\n",
       " 'compute',\n",
       " 'perform',\n",
       " 'Model',\n",
       " 'wise',\n",
       " 'identical',\n",
       " 'forward',\n",
       " 'embedding',\n",
       " 'dv',\n",
       " 'matrix',\n",
       " 'faster',\n",
       " 'outperforms',\n",
       " 'attend',\n",
       " 'dimensionality',\n",
       " 'probabilities',\n",
       " 'sentences',\n",
       " 'Training',\n",
       " 'Machine',\n",
       " 'high',\n",
       " 'Vinyals',\n",
       " 'Geoffrey',\n",
       " 'CoRR',\n",
       " 'abs',\n",
       " 'ACL',\n",
       " 'Sutskever',\n",
       " 'Tomas',\n",
       " 'introduced',\n",
       " 'sampling',\n",
       " 'natural',\n",
       " 'billion',\n",
       " 'France',\n",
       " 'objective',\n",
       " 'Noise',\n",
       " 'analogy',\n",
       " 'Montreal',\n",
       " 'context',\n",
       " 'Hierarchical',\n",
       " 'distribution',\n",
       " 'Mnih',\n",
       " 'Huffman',\n",
       " 'Sampling',\n",
       " 'signiﬁcantly',\n",
       " 'convolutional',\n",
       " 'including',\n",
       " 'days',\n",
       " 'parsing',\n",
       " 'order',\n",
       " 'Systems',\n",
       " 'Recurrent',\n",
       " 'short',\n",
       " 'term',\n",
       " 'sequences',\n",
       " 'steps',\n",
       " 'hidden',\n",
       " 'longer',\n",
       " 'case',\n",
       " 'stack',\n",
       " 'residual',\n",
       " 'pairs',\n",
       " 'computed',\n",
       " 'Dot',\n",
       " 'consists',\n",
       " 'space',\n",
       " 'dimensional',\n",
       " 'pos',\n",
       " 'range',\n",
       " 'improve',\n",
       " 'exhibit',\n",
       " 'source',\n",
       " 'rate',\n",
       " 'Deep',\n",
       " 'dropout',\n",
       " 'reported',\n",
       " 'beam',\n",
       " 'experiments',\n",
       " 'Journal',\n",
       " 'discriminative',\n",
       " 'Luong',\n",
       " 'text',\n",
       " 'Quoc',\n",
       " 'Empirical',\n",
       " 'Advances',\n",
       " 'Ilya',\n",
       " 'International',\n",
       " 'Representations',\n",
       " 'rare',\n",
       " 'Weston',\n",
       " 'Layer5',\n",
       " 'making',\n",
       " 'Mountain',\n",
       " 'View',\n",
       " 'efﬁcient',\n",
       " 'Contrastive',\n",
       " 'Estimation',\n",
       " 'capital',\n",
       " 'wt',\n",
       " 'vwI',\n",
       " 'frequency',\n",
       " 'noise',\n",
       " 'Brain',\n",
       " 'Noam',\n",
       " 'recurrence',\n",
       " 'ensembles',\n",
       " 'fraction',\n",
       " 'implemented',\n",
       " 'parameter',\n",
       " 'performed',\n",
       " 'Long',\n",
       " 'factor',\n",
       " 'achieved',\n",
       " 'distance',\n",
       " 'P100',\n",
       " 'ConvS2S',\n",
       " 'parallel',\n",
       " 'difficult',\n",
       " 'reduced',\n",
       " 'averaging',\n",
       " 'Head',\n",
       " 'step',\n",
       " 'point',\n",
       " 'feed',\n",
       " 'employ',\n",
       " 'connections',\n",
       " 'outputs',\n",
       " 'query',\n",
       " 'sum',\n",
       " 'Scaled',\n",
       " 'Product',\n",
       " 'compatibility',\n",
       " 'products',\n",
       " 'apply',\n",
       " 'algorithm',\n",
       " 'larger',\n",
       " 'times',\n",
       " 'typical',\n",
       " 'property',\n",
       " 'applied',\n",
       " 'restricted',\n",
       " 'encodings',\n",
       " 'encoding',\n",
       " 'compare',\n",
       " 'maximum',\n",
       " 'pair',\n",
       " 'individual',\n",
       " 'consisting',\n",
       " 'root',\n",
       " 'improves',\n",
       " 'chosen',\n",
       " 'development',\n",
       " 'Zhu',\n",
       " 'Minh',\n",
       " 'Computational',\n",
       " 'accurate',\n",
       " 'Linguistics',\n",
       " 'Chen',\n",
       " 'good',\n",
       " 'result',\n",
       " 'Paris',\n",
       " 'Word',\n",
       " 'Boston',\n",
       " 'techniques',\n",
       " 'test',\n",
       " 'Volga',\n",
       " 'River',\n",
       " 'Germany',\n",
       " 'Berlin',\n",
       " 'formulation',\n",
       " 'ﬁrst',\n",
       " 'node',\n",
       " 'σ',\n",
       " 'Collobert',\n",
       " 'samples',\n",
       " 'analogies',\n",
       " 'closest',\n",
       " 'Airlines',\n",
       " 'Lufthansa',\n",
       " 'works',\n",
       " 'Shazeer',\n",
       " 'Jakob',\n",
       " 'Łukasz',\n",
       " 'performing',\n",
       " 'improving',\n",
       " 'constituency',\n",
       " 'random',\n",
       " 'replacing',\n",
       " 'idea',\n",
       " 'involved',\n",
       " 'free',\n",
       " 'codebase',\n",
       " 'tensor2tensor',\n",
       " 'inference',\n",
       " 'gated',\n",
       " 'approaches',\n",
       " 'problems',\n",
       " 'symbol',\n",
       " 'lengths',\n",
       " 'tion',\n",
       " 'ByteNet',\n",
       " 'basic',\n",
       " 'computing',\n",
       " 'required',\n",
       " 'linearly',\n",
       " 'constant',\n",
       " 'convolution',\n",
       " 'continuous',\n",
       " 'element',\n",
       " 'fully',\n",
       " 'connected',\n",
       " 'composed',\n",
       " 'normalization',\n",
       " 'prevent',\n",
       " 'practice',\n",
       " 'commonly',\n",
       " 'additive',\n",
       " 'code',\n",
       " 'magnitude',\n",
       " 'extremely',\n",
       " 'Rdmodel',\n",
       " 'total',\n",
       " 'parameters',\n",
       " 'kernel',\n",
       " 'types',\n",
       " 'Layer',\n",
       " 'relative',\n",
       " 'chose',\n",
       " 'easily',\n",
       " 'tional',\n",
       " 'paths',\n",
       " 'combination',\n",
       " 'byte',\n",
       " 'plan',\n",
       " 'increasing',\n",
       " 'considerably',\n",
       " 'distributions',\n",
       " 'syntactic',\n",
       " 'semantic',\n",
       " 'standard',\n",
       " 'target',\n",
       " 'formula',\n",
       " 'warmup_steps',\n",
       " 'Ensemble',\n",
       " 'Pdrop',\n",
       " 'published',\n",
       " 'search',\n",
       " 'rows',\n",
       " 'observe',\n",
       " 'suggests',\n",
       " 'replace',\n",
       " 'Petrov',\n",
       " 'Dyer',\n",
       " 'achieve',\n",
       " 'Kyunghyun',\n",
       " 'Cho',\n",
       " 'Thang',\n",
       " 'statistical',\n",
       " 'NAACL',\n",
       " 'David',\n",
       " 'Alex',\n",
       " 'Zhang',\n",
       " 'IEEE',\n",
       " 'Methods',\n",
       " 'Natural',\n",
       " 'Oriol',\n",
       " 'ICLR',\n",
       " 'Christopher',\n",
       " 'Manning',\n",
       " 'corpus',\n",
       " 'Richard',\n",
       " 'Socher',\n",
       " 'deep',\n",
       " 'Annual',\n",
       " 'Meeting',\n",
       " 'Dean',\n",
       " 'Jason',\n",
       " 'fast',\n",
       " 'Volume',\n",
       " 'attentions',\n",
       " 'Compositionality',\n",
       " 'distributed',\n",
       " 'precise',\n",
       " 'speedup',\n",
       " 'negative',\n",
       " 'processing',\n",
       " 'interesting',\n",
       " 'Spain',\n",
       " 'meaning',\n",
       " 'recursive',\n",
       " 'Canadiens',\n",
       " 'Maple',\n",
       " 'Leafs',\n",
       " 'nearest',\n",
       " 'Russia',\n",
       " 'river',\n",
       " 'close',\n",
       " 'maximize',\n",
       " 'exp',\n",
       " 'vw',\n",
       " 'nodes',\n",
       " 'binary',\n",
       " 'assigns',\n",
       " 'provide',\n",
       " 'sample',\n",
       " 'unigram',\n",
       " 'frequently',\n",
       " 'stands',\n",
       " 'threshold',\n",
       " 'news',\n",
       " 'York',\n",
       " 'word2vec',\n",
       " 'Examples',\n",
       " 'formed',\n",
       " 'count',\n",
       " 'ing',\n",
       " 'Vector',\n",
       " 'comparison',\n",
       " 'Turian',\n",
       " 'Redmond',\n",
       " 'Havel',\n",
       " 'probabilistic',\n",
       " 'international',\n",
       " 'Speech',\n",
       " 'Association',\n",
       " 'solely',\n",
       " 'Ashish',\n",
       " 'Niki',\n",
       " 'Uszkoreit',\n",
       " 'Llion',\n",
       " 'Aidan',\n",
       " 'University',\n",
       " 'Illia',\n",
       " 'Abstract',\n",
       " 'complex',\n",
       " 'connect',\n",
       " 'propose',\n",
       " 'existing',\n",
       " 'costs',\n",
       " 'literature',\n",
       " 'generalizes',\n",
       " 'limited',\n",
       " 'contribution',\n",
       " 'proposed',\n",
       " 'RNNs',\n",
       " 'designed',\n",
       " 'scaled',\n",
       " 'evaluated',\n",
       " 'countless']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data = sorted(bag_words.items(), key=lambda bag_words: bag_words[1], reverse=True)\n",
    "\n",
    "most_used = sorted_data[:500]\n",
    "most_used = [ pair[0] for pair in most_used]\n",
    "most_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff444f92-6f71-409c-be0a-7460c9a6ee79",
   "metadata": {},
   "source": [
    "### Step 3: Generate vectors\n",
    "For each word calculate how many times it were in the text and compose a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e97eea7-6fe0-4421-8c80-aea0c90260d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35,  0, 43, 23, 38, 40,  1,  3,  2,  4, 33, 32,  0, 13, 25, 27,  0,\n",
       "        2,  0, 12,  0, 13, 24,  1, 13,  1, 15, 16,  2,  0,  4,  0,  0, 14,\n",
       "       11,  9, 18,  7,  2, 15,  3,  1, 17,  7,  2,  3,  9,  9,  9,  8,  1,\n",
       "        8, 10,  6,  8,  2,  5, 12, 14,  2,  0,  3,  3,  0, 11,  6, 12,  5,\n",
       "        0,  0,  3,  8,  5,  0,  5,  9,  0,  0,  6, 11, 11, 11,  4,  0,  2,\n",
       "        0,  5,  3,  3,  4,  1,  2,  0,  0, 10, 10,  1,  0,  1,  2,  1,  5,\n",
       "        0,  2,  6,  0,  6,  0,  0,  5,  1,  4,  1,  9,  6,  7,  0,  0,  1,\n",
       "        4,  0,  2,  4,  0,  0,  0,  0,  8,  0,  3,  2,  0,  2,  0,  2,  0,\n",
       "        1,  0,  1,  6,  3,  5,  4,  6,  6,  6,  7,  0,  0,  0,  3,  2,  0,\n",
       "        1,  0,  1,  1,  2,  2,  0,  0,  0,  0,  2,  3,  2,  0,  2,  5,  1,\n",
       "        1,  4,  5,  0,  3,  0,  0,  0,  1,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  0,  1,  1,  0,  1,  0,  0,  2,\n",
       "        1,  0,  0,  0,  2,  0,  0,  0,  0,  3,  0,  2,  4,  3,  0,  2,  3,\n",
       "        3,  3,  1,  0,  0,  2,  0,  3,  3,  0,  0,  4,  0,  2,  1,  1,  2,\n",
       "        3,  4,  4,  0,  0,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  0,  0,\n",
       "        0,  0,  1,  0,  2,  0,  0,  0,  3,  1,  0,  0,  0,  0,  1,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  3,  1,  3,  2,\n",
       "        3,  3,  0,  0,  0,  2,  0,  1,  3,  2,  3,  3,  3,  0,  0,  1,  2,\n",
       "        2,  3,  2,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  2,  0,  0,  0,  1,  0,  0,  1,  0,  2,\n",
       "        0,  2,  0,  0,  0,  0,  1,  1,  0,  0,  2,  0,  2,  2,  1,  0,  0,\n",
       "        0,  2,  2,  0,  0,  0,  0,  0,  1,  0,  1,  1,  2,  2,  0,  0,  1,\n",
       "        0,  0,  0,  1,  1,  2,  1,  0,  1,  0,  0,  1,  2,  2,  2,  2,  2,\n",
       "        1,  2,  0,  0,  0,  1,  1,  0,  0,  2,  1,  0,  0,  2,  0,  0,  0,\n",
       "        2,  1,  1,  0,  0,  2,  1,  1,  0,  1,  2,  2,  2,  2,  2,  2,  1,\n",
       "        1,  2,  2,  2,  2,  0,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        0,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0,  1,  0,  0,  0,  1,\n",
       "        1,  0,  0,  0,  0,  1,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector_dist = np.zeros(500, dtype=int)\n",
    "for word in words_dist:\n",
    "    if word in most_used:\n",
    "        index = most_used.index(word)\n",
    "        vector_dist[index] += 1\n",
    "vector_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e47b9cd7-b451-4f77-a7ac-384efb861719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 75, 22, 37,  9,  6, 39, 33, 33, 29,  0,  0, 29, 15,  2,  0, 26,\n",
       "       24, 25, 13, 24, 11,  0, 22, 10, 22,  7,  6, 18, 20, 15, 19, 18,  4,\n",
       "        7,  9,  0, 10, 15,  2, 14, 16,  0,  9, 14, 13,  7,  7,  7,  8, 15,\n",
       "        7,  5,  9,  6, 12,  9,  2,  0, 11, 13, 10,  9, 12,  1,  6,  0,  6,\n",
       "       11, 11,  8,  3,  6, 11,  6,  2, 11, 11,  5,  0,  0,  0,  6, 10,  8,\n",
       "       10,  5,  7,  7,  6,  9,  8, 10, 10,  0,  0,  8,  9,  8,  7,  8,  4,\n",
       "        9,  7,  3,  9,  3,  9,  9,  4,  8,  5,  8,  0,  2,  1,  8,  8,  7,\n",
       "        4,  8,  6,  4,  8,  8,  8,  8,  0,  7,  4,  5,  7,  5,  7,  5,  7,\n",
       "        6,  7,  6,  1,  4,  2,  3,  1,  1,  1,  0,  6,  6,  6,  3,  4,  6,\n",
       "        5,  6,  5,  5,  4,  4,  6,  6,  6,  6,  4,  3,  4,  6,  4,  1,  5,\n",
       "        5,  2,  1,  6,  3,  6,  6,  6,  5,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  4,  4,  5,  4,  5,  5,  3,\n",
       "        4,  5,  5,  5,  3,  5,  5,  5,  5,  2,  5,  3,  1,  2,  5,  3,  2,\n",
       "        2,  2,  4,  5,  5,  3,  5,  2,  2,  5,  5,  1,  5,  3,  4,  4,  3,\n",
       "        2,  1,  1,  5,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  4,\n",
       "        4,  4,  3,  4,  2,  4,  4,  4,  1,  3,  4,  4,  4,  4,  3,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  1,  4,  4,  4,  4,  4,  1,  3,  1,  2,\n",
       "        1,  1,  4,  4,  4,  2,  4,  3,  1,  2,  1,  1,  1,  4,  4,  3,  2,\n",
       "        2,  1,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1,  3,  3,  3,  2,  3,  3,  2,  3,  1,\n",
       "        3,  1,  3,  3,  3,  3,  2,  2,  3,  3,  1,  3,  1,  1,  2,  3,  3,\n",
       "        3,  1,  1,  3,  3,  3,  3,  3,  2,  3,  2,  2,  1,  1,  3,  3,  2,\n",
       "        3,  3,  3,  2,  2,  1,  2,  3,  2,  3,  3,  2,  1,  1,  1,  1,  1,\n",
       "        2,  1,  3,  3,  3,  2,  2,  3,  3,  1,  2,  3,  3,  1,  3,  3,  3,\n",
       "        1,  2,  2,  3,  3,  1,  2,  2,  3,  2,  1,  1,  1,  1,  1,  1,  2,\n",
       "        2,  1,  1,  1,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        2,  2,  2,  2,  2,  2,  1,  2,  1,  1,  2,  2,  1,  2,  2,  2,  1,\n",
       "        1,  2,  2,  2,  2,  1,  2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_att = np.zeros(500, dtype=int)\n",
    "for word in words_att:\n",
    "    if word in most_used:\n",
    "        index = most_used.index(word)\n",
    "        vector_att[index] += 1\n",
    "vector_att"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc9147-53bf-4eb6-ac80-898916def1d5",
   "metadata": {},
   "source": [
    "Calculations of cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ba412fa-cf82-43ed-a156-e1eb8eb3eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57cf171-3fd5-4cff-84f3-581f7b98e763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3254797972342878"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(vector_att, vector_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdffb1a-d231-477a-a071-150ad170035d",
   "metadata": {},
   "source": [
    "### Step 4: Interpret measured cosine similarity\n",
    "#### Cosine similarity:\n",
    "\n",
    "1 means the vectors are identical\n",
    "\n",
    "0 means the vectors are orthogonal (no similarity)\n",
    "\n",
    "-1 means the vectors are diametrically opposed (completely dissimilar)\n",
    "\n",
    "Therefore, the similarity is relatively low. The texts are not very similar, but there is still some overlap in the vocabulary or context. These two documents would indicate that they share some thematic elements but also have differences in wording and some unique content not shared by the other document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af774502-2686-482f-86af-da572b1560a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I really like fish, it reminds me of Innopolis, Tomatoes is ok, hard to forget Penza. Wish to forget, Sometimes I visit concert, reminds of how hard to get things done, This course is lame. Some fish write this tomatoes lectures. Sometimes Im thinking about Penza\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66f73958-3484-4ee2-bd61-4d3166d664c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = re.findall(r'\\b(?!\\d+\\b)\\w+\\b', text)\n",
    "s1 = re.findall(r'\\b(?!\\d+\\b)\\w+\\b',\"Tomatoes is ok, hard to forget Penza. Wish to forget.\")\n",
    "s2 = re.findall(r'\\b(?!\\d+\\b)\\w+\\b', \"Sometimes I visit concert, reminds of how hard to get things done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c8312ad-d4f1-408b-a7ff-3defc9659326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'really',\n",
       " 'like',\n",
       " 'fish',\n",
       " 'it',\n",
       " 'reminds',\n",
       " 'me',\n",
       " 'of',\n",
       " 'Innopolis',\n",
       " 'Tomatoes',\n",
       " 'is',\n",
       " 'ok',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'forget',\n",
       " 'Penza',\n",
       " 'Wish',\n",
       " 'Sometimes',\n",
       " 'visit',\n",
       " 'concert',\n",
       " 'how',\n",
       " 'get',\n",
       " 'things',\n",
       " 'done',\n",
       " 'This',\n",
       " 'course',\n",
       " 'lame',\n",
       " 'Some',\n",
       " 'write',\n",
       " 'this',\n",
       " 'tomatoes',\n",
       " 'lectures',\n",
       " 'Im',\n",
       " 'thinking',\n",
       " 'about']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words = {}\n",
    "for word in check:\n",
    "    if word in bag_words:\n",
    "        bag_words[word] += 1\n",
    "    else:\n",
    "        bag_words[word] = 1\n",
    "\n",
    "most_used = [ pair for pair in bag_words.keys()]\n",
    "most_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4561e74f-85e1-4b73-a24f-f7411ab54381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch1 = np.zeros(len(most_used), dtype=int)\n",
    "for word in s1:\n",
    "    if word in most_used:\n",
    "        index = most_used.index(word)\n",
    "        ch1[index] += 1\n",
    "ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2166ce50-a69d-4186-9ab2-96cf8f386802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch2 = np.zeros(len(most_used), dtype=int)\n",
    "for word in s2:\n",
    "    if word in most_used:\n",
    "        index = most_used.index(word)\n",
    "        ch2[index] += 1\n",
    "ch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7936ac2-cfc0-483b-aacf-36afc7c42eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(ch1,ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39a32cfe-2a69-41fa-9e69-e684fd387a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7416573867739413"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a146a941-a782-4731-bd1f-314bb41927e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4641016151377544"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b427abc-c558-4736-97db-aacddf63c93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23145502494313788"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(ch1,ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "837cd55f-00d1-4b13-85cd-304443cba85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23183209174368644"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3/(3.74*3.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebffd34c-54e0-4ce4-a9c5-1b101b8fca2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958d5ee-bbd8-44d0-bb44-76218cc6b9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
